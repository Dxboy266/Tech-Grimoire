# Kafka核心知识点与大厂面试题全解

> **适用场景**：大厂Java后端面试（字节、美团、阿里、腾讯等）  
> **难度等级**：⭐⭐⭐⭐⭐（高级开发必备）  
> **作者经验**：3年实战，DFN项目中使用Kafka异步解耦，吞吐量提升3倍，统一认证项目453w+认证服务

---

## 文档说明

本文档基于以下资料整理：
- 《Kafka扩展_文档整理.md》- 核心架构与原理
- 《Kafka扩展_数据存储.md》- 底层存储机制
- 《Kafka扩展_水印机制.md》- 水印备份机制
- 《Kafka核心知识点与面试题全解.md》- 面试题参考
- 互联网真实大厂面试题

**特点**：
- ✅ 由浅入深，系统全面
- ✅ 偏重底层原理（存储结构、零拷贝、水印机制）
- ✅ 50道大厂真实面试题，标注出处
- ✅ 结合项目经验（DFN项目、统一认证项目）
- ✅ 关键配置和伪代码示例

---

## 目录

1. [Kafka基础入门](#一kafka基础入门)
2. [核心架构原理](#二核心架构原理)
3. [底层存储机制](#三底层存储机制)⭐核心
4. [水印备份机制](#四水印备份机制)⭐核心
5. [生产者原理](#五生产者原理)
6. [消费者原理](#六消费者原理)
7. [可靠性保证](#七可靠性保证)
8. [性能优化与实战](#八性能优化与实战)
9. [50道大厂面试题](#九50道大厂面试题)⭐核心

---

## 一、Kafka基础入门

### 1.1 Kafka概述

#### 什么是Kafka

Kafka是一个**分布式的基于发布/订阅模式的消息队列**，主要应用于大数据实时处理领域。

**历史背景**：
- 由LinkedIn公司开发（2010年）
- 使用Scala语言编写
- 2010年贡献给Apache基金会成为顶级开源项目
- 目标：为处理实时数据提供统一、高吞吐、低延迟的平台

**核心定位**：
- 分布式发布-订阅消息系统
- 分布式的、可划分的、冗余备份的持久性日志服务
- 可处理消费者在网站中的所有动作流数据

#### 为什么使用Kafka

##### 1. 多个生产者
Kafka可以无缝地支持多个生产者，不管客户端在使用单个主题还是多个主题，非常适合从多个前端系统收集数据，并以统一的格式对外提供数据。

##### 2. 多个消费者
Kafka支持多个消费者从一个单独的消息流上读取数据，而且消费者之间互不影响。

##### 3. 基于磁盘的数据存储
Kafka不仅支持多个消费者，还允许消费者非实时地读取消息，这要归功于Kafka的数据保留特性。消息被提交到磁盘，根据设置的保留规则进行保存。

##### 4. 伸缩性
Broker可以随着数据量不断增长，扩展到上百个，对在线集群进行扩展丝毫不影响整体系统的可用性。即使个别broker失效，仍然可以持续地为客户提供服务。

##### 5. 高性能
通过横向扩展生产者、消费者和broker，Kafka可以轻松处理巨大的消息流。

### 1.2 核心特性

> Kafka以高吞吐量著称，主要有以下特性：

- **高吞吐量、低延迟**：每秒可以处理几十万条消息，延迟最低只有几毫秒
- **可扩展性**：集群支持热扩展
- **持久性、可靠性**：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失
- **容错性**：允许集群中节点失败（若副本数量为n，则允许n-1个节点失败）
- **高并发**：支持数千个客户端同时读写

### 1.3 设计思想

#### Consumer Group（消费者组）
各个consumer可以组成一个组，每个消息只能被组中的一个consumer消费。如果一个消息可以被多个consumer消费，那么这些consumer必须在不同的组。

#### 消息状态
在Kafka中，消息的状态被保存在consumer中，broker不会关心哪个消息被消费了被谁消费了，只记录一个offset值（指向partition中下一个要被消费的消息位置）。

#### 消息持久化
Kafka中会把消息持久化到本地文件系统中，并且保持极高的效率。

#### 消息有效期
Kafka会长久保留其中的消息，以便consumer可以多次消费，当然其中很多细节是可配置的。

#### 批量发送
Kafka支持以消息集合为单位进行批量发送，以提高push效率。

#### 集群模式
Kafka集群中broker之间的关系不是主从关系，各个broker在集群中地位一样，可以随意增加或删除任何一个broker节点。

#### 负载均衡
Kafka提供了metadata API来管理broker之间的负载。

#### 同步异步
Producer采用异步push方式，极大提高Kafka系统的吞吐率（可以通过参数控制是采用同步还是异步方式）。

#### 分区机制（Partition）
Kafka的broker端支持消息分区，Producer可以决定把消息发到哪个分区。在一个分区中消息的顺序就是Producer发送消息的顺序，一个主题中可以有多个分区。

#### 离线数据装载
Kafka由于对可拓展的数据持久化的支持，非常适合向Hadoop或者数据仓库中进行数据装载。

### 1.4 应用场景

Apache Kafka能够支撑海量数据的数据传递，在离线和实时的消息处理业务系统中都有广泛应用。

#### 1. 活动跟踪
Kafka最初的使用场景是跟踪用户的活动。网站用户与前端应用程序发生交互，前端应用程序生成用户活动相关的消息（页面访问次数、点击量、添加用户资料等），这些消息被发布到一个或多个主题上，由后端应用程序负责读取。

#### 2. 消息传递
Kafka可以作为传统消息中间件的替代方案，用于应用程序之间的消息传递。

#### 3. 度量指标
Kafka可以用于收集应用程序和系统度量指标以及日志。应用程序定期把度量指标发布到Kafka主题上，监控系统或告警系统读取这些消息。

#### 4. 日志记录
日志消息可以被发布到Kafka主题上，然后被路由到专门的日志搜索系统（如Elasticsearch）或安全分析应用程序。

#### 5. 流处理
流处理框架可以从Kafka读取数据，进行实时处理，然后将结果写回Kafka或其他系统。可以编写小型应用程序来操作Kafka消息，比如计算度量指标、为其他应用程序有效地处理消息分区、或者对来自多个数据源的消息进行转换。

### 1.5 Kafka vs RabbitMQ

**结合项目经验对比**：

| 特性 | Kafka | RabbitMQ | 选择建议 |
|------|-------|----------|---------|
| **吞吐量** | 百万级/秒 ⭐⭐⭐⭐⭐ | 万级/秒 ⭐⭐⭐ | 高吞吐量选Kafka |
| **延迟** | 毫秒级（10ms） | 微秒级（更低） | 低延迟选RabbitMQ |
| **消息堆积** | 支持TB级 ⭐⭐⭐⭐⭐ | 支持GB级 ⭐⭐ | 大量堆积选Kafka |
| **顺序性** | 分区内有序 ⭐⭐⭐⭐ | 队列内有序 ⭐⭐⭐⭐ | 都支持 |
| **消息回溯** | 支持 ⭐⭐⭐⭐⭐ | 不支持 | 需要回溯选Kafka |
| **延迟队列** | 不支持（需自己实现） | 原生支持 ⭐⭐⭐⭐⭐ | 延迟队列选RabbitMQ |
| **死信队列** | 需自己实现 | 原生支持 ⭐⭐⭐⭐⭐ | 死信队列选RabbitMQ |
| **适用场景** | 日志收集、大数据、流式处理 | 业务解耦、延迟队列、死信队列 | 根据场景选择 |

**项目经验**：
- **DFN项目**：1000+门店，高并发认证请求，需要高吞吐量 → 选择Kafka
- **统一认证项目**：异步解耦下游10+系统，响应时间从2秒→500ms，吞吐量提升3倍 → 使用Kafka

**选择建议**：
- 选Kafka：高吞吐量、消息堆积、日志收集、大数据、流式处理
- 选RabbitMQ：业务解耦、延迟队列、死信队列、低延迟、复杂路由

---

## 二、核心架构原理

### 2.1 Kafka架构图

```
┌─────────────────────────────────────────────────┐
│                   Kafka Cluster                  │
│  ┌─────────┐  ┌─────────┐  ┌─────────┐         │
│  │Broker 0 │  │Broker 1 │  │Broker 2 │         │
│  │(Leader) │  │(Follower│  │(Follower│         │
│  └─────────┘  └─────────┘  └─────────┘         │
│       ↑            ↑            ↑               │
│       │            │            │               │
│   ┌───┴────────────┴────────────┴───┐          │
│   │      ZooKeeper / KRaft          │          │
│   └─────────────────────────────────┘          │
└─────────────────────────────────────────────────┘
         ↑                        ↓
    Producer                  Consumer Group
```

### 2.2 核心组件

#### 核心术语（面试必问）

| 术语 | 说明 | 类比理解 |
|------|------|----------|
| **Broker** | Kafka服务器节点 | 数据库的一个实例 |
| **Topic** | 消息主题/分类 | MySQL的一张表 |
| **Partition** | Topic的物理分区 | 表的分区表 |
| **Offset** | 消息在分区中的位置 | 自增主键ID |
| **Replica** | 分区的副本 | MySQL的主从复制 |
| **Leader** | 主副本，负责读写 | MySQL的Master |
| **Follower** | 从副本，只负责同步 | MySQL的Slave |
| **ISR** | In-Sync Replicas，同步副本集合 | 延迟低的从库 |
| **Consumer Group** | 消费者组 | 负载均衡的消费者集群 |

#### Broker
Kafka集群包含一个或多个服务器，这种服务器被称为broker。

**特点**：
- broker端不维护数据的消费状态，提升了性能
- 直接使用磁盘进行存储，线性读写，速度快
- 避免了数据在JVM内存和系统内存之间的复制
- 减少耗性能的创建对象和垃圾回收

#### Producer（生产者）
消息的发送方，负责发布消息到Kafka broker。

#### Consumer（消费者）
消息消费者，向Kafka broker读取消息的客户端。consumer从broker拉取(pull)数据并进行处理。

#### Consumer Group（消费者组）
同样是逻辑上的概念，是Kafka实现单播和广播两种消息模型的手段。

**特点**：
- 同一个topic的数据，会广播给不同的group
- 同一个group中的worker，只有一个worker能拿到这个数据
- 对于同一个topic，每个group都可以拿到同样的所有数据
- 数据进入group后只能被其中的一个worker消费
- worker的数量通常不超过partition的数量

#### Topic（主题）
是Kafka下消息的类别，类似于RabbitMQ中的Exchange的概念。

**特点**：
- Kafka中的消息以主题为单位进行归类
- 生产者负责将消息发送到特定的主题
- 消费者负责订阅主题并进行消费
- 物理上不同Topic的消息分开存储
- 逻辑上一个Topic的消息虽然保存于一个或多个broker上，但用户只需指定消息的Topic即可

#### Partition（分区）
是Kafka下数据存储的基本单元，这个是物理上的概念。

**特点**：
- 同一个topic的数据，会被分散的存储到多个partition中
- 这些partition可以在同一台机器上，也可以是在多台机器上
- 有利于水平扩展，避免单台机器在磁盘空间和性能上的限制
- 通过复制来增加数据冗余性，提高容灾能力
- partition的数量通常是Broker Server数量的整数倍

#### Replication（副本）
每一个分区都有多个副本，副本的作用是做备胎。

**特点**：
- 主分区（Leader）会将数据同步到从分区（Follower）
- 当主分区（Leader）故障时会选择一个备胎（Follower）上位，成为Leader
- 在kafka中默认副本的最大数量是10个
- 副本的数量不能大于Broker的数量
- follower和leader绝对是在不同的机器
- 同一机器对同一个分区也只可能存放一个副本

#### Zookeeper
kafka集群依赖zookeeper来保存集群的元信息，来保证系统的可用性。

**存储内容**：
- consumers：存储Consumer的相关信息（新版本已迁移到Kafka）
- admin：存储Kafka集群的管理信息
- config：存储Kafka相关的配置信息
- controller：存储Controller中央控制器所在broker的信息
- brokers：存储Kafka集群broker的相关信息
- isr_change_notification：存储ISR变更通知信息

### 2.3 消费模型

消息由生产者发送到kafka集群后，会被消费者消费。一般来说消费模型有两种：**推送模型(push)**和**拉取模型(pull)**。

**明确**：在讨论推拉模式时，指的是Consumer和Broker之间的交互。Producer与Broker之间默认是推的方式。

#### 推模式（Push）

推模式指的是消息从Broker推向Consumer，即Consumer被动的接收消息，由Broker来主导消息的发送。

**优点**：
- 消息实时性高，Broker接受完消息之后可以立马推送给Consumer
- 对于消费者使用来说更简单，等着消息推过来即可

**缺点**：
- 推送速率难以适应消费速率
- 当生产者往Broker发送消息的速率大于消费者消费消息的速率时，随着时间增长会把消费者撑死
- 不同的消费者的消费速率不一样，Broker很难平衡每个消费者的推送速率
- 如果要实现自适应的推送速率，需要在推送时消费者告诉Broker，Broker需要维护每个消费者的状态

#### 拉模式（Pull）

拉模式指的是Consumer主动向Broker请求拉取消息，即Broker被动的发送消息给Consumer。

**优点**：
- 主动权在消费者身上，消费者可以根据自身情况来发起拉取消息的请求
- 消费者可以根据一定的策略停止拉取，或者间隔拉取
- Broker相对轻松，只管存生产者发来的消息，消费者来请求就给消息
- 更合适进行消息的批量发送，可以参考消费者请求的信息来决定缓存多少消息后批量发送

**缺点**：
- 消息延迟，消费者不知道消息何时到达，只能不断地拉取
- 需要降低请求的频率（如隔2秒请求一次），可能导致消息延迟2秒
- 消息忙请求，如果消息隔了几个小时才有，那么在几个小时之内消费者的请求都是无效的

#### Kafka是什么模式

**Kafka选择了拉模式**（RocketMQ也是拉模式）。

**原因**：
- 消费者各种各样，Broker不应该有依赖于消费者的倾向
- Broker已经为你保存好消息了，你要就来拿
- Broker不会成为瓶颈，因为消费端有业务消耗比较慢
- Broker是一个中心点，能轻量就尽量轻量

#### 长轮询模式

Kafka在拉请求中有参数，可以使得消费者请求在"长轮询"中阻塞等待。

**原理**：
- 消费者去Broker拉消息，定义了一个超时时间
- 如果有消息马上返回消息
- 如果没有消息消费者等着直到超时，然后再次发起拉消息请求
- Broker配合：如果消费者请求过来，有消息马上返回，没有消息建立一个延迟操作，等条件满足了再返回

**配置示例**：
```properties
# 一次poll最大拉取消息的条数
max.poll.records=500

# 长轮询时间
# 如果一次poll到500条消息，直接执行消费
# 如果一次没有poll到500条消息，其时间在1秒内，继续poll
# 如果时间到达1s，无论是否poll到了500条消息，都直接执行消费
```

```java
while (true) {
    // poll() API 是拉取消息的长轮询
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
    for (ConsumerRecord<String, String> record : records) {
        System.out.printf("收到消息：partition = %d,offset = %d, key =%s, value = %s%n", 
            record.partition(), record.offset(), record.key(), record.value());
    }
}
```

### 2.4 副本机制（Replication）

又称为备份机制，通常是指在分布式系统中在多台机器中存储相同的数据进行备份的机制。

#### 副本的作用

副本机制主要有3个好处：

1. **提供数据冗余**：即使部分机器出现故障，系统仍然可以提供服务，增加了整体的可用性和数据持久化
2. **提供高伸缩性**：支持横向扩展，可以通过增加副本数来提供读性能
3. **改善数据局部性**：允许将数据放入与用户地理位置相近的地方，从而降低系统延时

**注意**：Kafka的副本机制只提供了第一个特点，即提供数据冗余的特性。

#### 副本定义

在Kafka中，一个主题下面可以有多个分区（partition），每个分区（partition）可以有多个副本。**Kafka的副本是以分区为维度进行划分的**。

**特点**：
- 同一个分区下的所有副本保存有相同的消息序列
- 这些副本分散保存在不同的Broker上，从而能够对抗部分Broker宕机带来的数据不可用
- 在生产环境中，每个分区的副本分布在不同的机器上

```
Topic: order-topic, Partition: 0, Replication-Factor: 3

Broker 0 (Leader)     Broker 1 (Follower)   Broker 2 (Follower)
┌──────────────┐      ┌──────────────┐      ┌──────────────┐
│ Partition 0  │      │ Partition 0  │      │ Partition 0  │
│  (Leader)    │◄─────│  (Follower)  │◄─────│  (Follower)  │
│              │      │              │      │              │
│ offset: 1000 │      │ offset: 998  │      │ offset: 995  │
└──────────────┘      └──────────────┘      └──────────────┘
      ↑                     ↑                     ↓
      │                     │                  Out of ISR
   Producer            In ISR (同步)         (延迟太大)
```

#### 副本的意义

在Kafka中，副本由一个leader节点和多个follower节点组成：

**Leader副本**：
- 负责接收消息
- 负责消费消息
- 提供读写服务

**Follower副本**：
- 既不提供写服务也不提供读服务
- 仅仅用于同步leader副本的消息
- 唯一作用：当leader副本出现问题时，通过ZooKeeper提供的监控功能实时感知到，并立即开启新一轮的领导者选举

#### 为什么这样设计

1. **方便实现读写一致**：
   - 只在leader副本上进行读写操作
   - 生产者写入什么消息，消费者就能读到什么消息
   - 消费者不会从follower上进行读取操作
   - 避免了主从同步过程中的延迟问题

2. **方便实现单调读（Monotonic Reads）**：
   - 在进行多次消费时，不会存在某条消息一会存在，一会消失的情况
   - 如果2个追随者副本F1和F2异步地拉取领导者副本数据
   - 假设F1拉取了Leader的最新消息而F2还未及时拉取
   - 如果消费者先从F1读取消息之后又从F2拉取消息，会出现数据不一致

3. **Kafka不需要读写分离**：
   - Kafka本身的设计是通过分区来分担集群读写的压力
   - 从架构层面可以很好的进行水平扩展
   - 读写分离更多的是一种弥补架构设计不足的方法

#### 副本角色

副本主要由三种角色组成：

- **AR（Assigned Replica）**：所有副本的统称，AR = ISR + OSR
- **ISR（In-Sync Replica）**：同步中的副本，可以参与leader选主，一旦落后太多（数量滞后和时间滞后两个维度）会被踢到OSR
- **OSR（Out-Sync Replicas）**：踢出同步的副本，一直追赶leader，追上后会进入ISR

### 2.5 ISR机制（⭐⭐⭐⭐⭐核心）

追随者副本不提供服务，只是定期地异步拉取领导者副本中的数据。

#### 什么是ISR

既然是异步的，就存在着不可能与Leader实时同步的风险。Kafka引入了**In-Sync Replicas（ISR）副本集合**。

**定义**：
- ISR中的副本都是与Leader同步的副本
- 不在ISR中的追随者副本就被认为是与Leader不同步的
- **Leader副本天然就在ISR中**
- ISR不只是追随者副本集合，它必然包括Leader副本
- 在某些情况下，ISR只有Leader这一个副本

#### 同步标准

Kafka判断Follower是否与Leader同步的标准，**不是看相差的消息数，而是看时间间隔**。

**核心参数**：`replica.lag.time.max.ms`
- 默认值：10秒
- 含义：Follower副本能够落后Leader副本的最长时间间隔
- 判断：只要一个Follower副本落后Leader副本的时间不连续超过10秒，就认为该Follower副本与Leader是同步的

**示例**：
```
Leader副本：写入了10条消息
Follower1副本：同步了6条消息
Follower2副本：同步了3条消息

问：哪个Follower与Leader不同步？
答：要根据具体情况来定（It depends）

关键不是看相差的消息数，而是看时间间隔：
- 如果Follower2在10秒内能追上，就认为是同步的
- 如果Follower2超过10秒还没追上，就会被踢出ISR
```

#### ISR动态调整

Follower副本唯一的工作就是不断地从Leader副本拉取消息，然后写入到自己的提交日志中。

**收缩ISR**：
- 如果同步过程的速度持续慢于Leader副本的消息写入速度
- 在`replica.lag.time.max.ms`时间后，此Follower副本会被认为是与Leader副本不同步的
- Kafka会自动收缩ISR集合，将该副本"踢出"ISR

**扩充ISR**：
- 如果该副本后面慢慢地追上了Leader的进度
- 它能够重新被加回ISR
- **ISR是一个动态调整的集合，而非静态不变的**

**动态调整机制**：
1. Kafka在启动时会开启两个任务
2. 任务一：定期检查是否需要缩减或扩大ISR集合
   - 周期：`replica.lag.time.max.ms`的一半（默认5000ms）
   - 检测到ISR集合中有失效副本时，收缩ISR集合
   - 检查到Follower的HighWatermark追赶上Leader时，扩充ISR
3. 任务二：周期性检查ISR变更记录
   - ISR集合发生变更时，变更记录会缓存到`isrChangeSet`中
   - 如果发现Set中有ISR集合的变更记录，在ZK中持久化一个节点
   - Controller在这个节点的路径上注册了Watcher，能够感知到ISR的变化
   - Controller向它所管理的broker发送更新元数据的请求
   - 最后删除该路径下已经处理过的节点

#### 不完全的首领选举

既然ISR是可以动态调整的，那么自然就可以出现ISR为空的情形。

**ISR为空的情况**：
- Leader副本天然就在ISR中
- 如果ISR为空了，说明Leader副本也"挂掉"了
- Kafka需要重新选举一个新的Leader

**非同步副本**：
- Kafka把所有不在ISR中的存活副本都称为非同步副本
- 非同步副本落后Leader太多
- 如果选择这些副本作为新Leader，可能出现数据的丢失
- 在Kafka中，选举这种副本的过程称为**不完全的首领选举**

**配置参数**：`unclean.leader.election.enable`
- 默认值：false（禁止不完全的首领选举）
- 含义：当首领副本挂掉且ISR中没有其他可用副本时，是否允许某个不完全同步的副本成为首领副本
- 影响：可能会导致数据丢失或数据不一致
- 场景：在某些对数据一致性要求较高的场景（如金融领域），这可能无法容忍
- 配置：如果能够允许部分数据不一致，可以配置为true

#### 最少同步副本

**配置参数**：`min.insync.replicas`
- 可以在broker或主题级别进行配置
- 含义：ISR列表中至少要有几个可用副本
- 示例：假设设置为2，当可用副本数量小于该值时，就认为整个分区处于不可用状态

#### 发送确认（acks参数）

Kafka在生产者上有一个可选的参数ack，该参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入成功。

**acks参数值**：

| acks值 | 说明 | 性能 | 可靠性 | 场景 |
|--------|------|------|--------|------|
| **acks=0** | 消息发送出去就认为已经成功，不等待任何来自服务器的响应 | 最高 | 最低（可能丢） | 日志收集 |
| **acks=1** | 只要集群的首领节点收到消息，生产者就会收到成功响应 | 中等 | 中等 | 一般业务 |
| **acks=all/-1** | 只有当所有参与复制的节点（ISR）全部收到消息时，生产者才会收到成功响应 | 最低 | 最高 | 金融、交易 |

**配置示例**：
```properties
# 生产者配置
acks=all  # 等待所有ISR副本确认
min.insync.replicas=2  # 至少2个ISR副本确认
retries=Integer.MAX_VALUE  # 无限重试
```

#### ISR的作用

1. **Leader选举范围**：
   - 当leader副本挂掉后，某个follower副本会被选为新的leader副本
   - 能够被选为leader副本的条件就是需要在ISR集合中
   - 可以通过参数`unclean.leader.election.enable`控制是否可以从非ISR集合中的副本选为leader节点

2. **生产ack为-1发送写入的数据**：
   - 生产者发送消息后，消息需要写入ISR集合中全部副本，才算提交成功
   - 如果ISR集合中只有leader一个节点，那么这个时候-1就退化为了1

**CAP权衡**：
- `unclean.leader.election.enable=true`：开启Unclean领导者选举，可能造成数据丢失，但提升了高可用性（A）
- `unclean.leader.election.enable=false`：禁止Unclean领导者选举，维护了数据的一致性（C），但牺牲了高可用性

### 2.6 Leader选举机制

#### 选举时机

1. Broker宕机（Controller检测到）
2. 分区Leader宕机
3. 手动触发（运维操作）

#### 选举策略

```
ISR = [Replica 0, Replica 1, Replica 2]

Leader宕机 → 从ISR中选举新Leader

选举规则：
1. 优先从ISR中选择第一个存活的Replica（AR列表顺序）
2. 如果ISR为空：
   - unclean.leader.election.enable=true → 允许非ISR选举（可能丢数据）
   - unclean.leader.election.enable=false → 等待ISR恢复（牺牲可用性）
```

**如何保证高可用**：
1. 设置`replication.factor >= 3`（至少3个副本）
2. 设置`min.insync.replicas=2`（至少2个副本确认写入）
3. 关闭`unclean.leader.election.enable`（防止数据丢失）

### 2.7 Controller机制

#### Controller是什么

**Controller**是Kafka集群的"大脑"，负责管理整个集群。从所有Broker中选举一个作为Controller。

#### Controller职责

1. **分区Leader选举**
2. **分区副本分配**
3. **监听Broker上下线**
4. **元数据管理**

**工作流程**：
```
Broker 0 (Controller)
    ↓
监听ZooKeeper/KRaft
    ↓
Broker 1宕机 → 触发Leader选举 → 通知其他Broker
```

---

## 三、底层存储机制（⭐⭐⭐⭐⭐核心）

### 3.1 消息存储结构

向Kafka的某个topic中写消息，实际上就是向内部的某个partition写消息，读取消息也是从某个partition上读取消息。

#### 目录结构

一个topic的实际物理存储对应着一个个的文件夹，文件夹名字按照`topic-num`进行目录划分：
- topic：topic名字
- num：partition编号，从0开始

**示例**：
```
/data/kafka/
  ├── order-topic-0/
  │   ├── 00000000000000000000.index
  │   ├── 00000000000000000000.log
  │   ├── 00000000000000000000.timeindex
  │   ├── 00000000000000170410.index
  │   ├── 00000000000000170410.log
  │   └── 00000000000000170410.timeindex
  ├── order-topic-1/
  └── order-topic-2/
```

#### LogSegment分段存储

partition文件夹下面不是一个日志文件，而是有多个文件。这些文件名一致的文件集合称为一个**LogSegment日志段文件组**。

**特点**：
- partition相当于一个巨型文件，被平均分配到多个大小相等的LogSegment数据文件中
- 每个LogSegment内部的消息数量不一定相等
- 这种分段存储的特性方便旧的LogSegment file快速被删除
- 同时加快了文件查找速度

**每个LogSegment主要包含**：
- 消息日志文件（.log结尾）
- 偏移量索引文件（.index结尾）
- 时间戳索引文件（.timeindex结尾）
- 快照文件（.snapshot结尾）

#### 基准偏移量（Base Offset）

每组LogSegment都有一个基准偏移量，用来表示当前LogSegment中第一条消息的offset。

**特点**：
- 消息offset是一个64位的长整形数
- 固定20位十进制整数，长度未达到用0进行填补
- 每个partition的消息offset单独维护
- 一个partition下的全局offset从0开始
- 每组LogSegment的基准偏移量就是上一组LogSegment的最大消息偏移量+1
- 每一个LogSegment的索引文件和日志文件都由自己的基准偏移量作为文件名
- 因此第一组LogSegment的索引文件和日志文件名都是`00000000000000000000`

### 3.2 相关存储文件

#### 数据日志文件（.log）

log文件作为日志文件，存储了消息内容以及该消息相关的元数据。

**写入方式**：
- Kafka会将消息以追加的方式顺序持久化到partition下面的最新的日志段下面的日志文件中
- 即只有最后一个LogSegment文件才能执行写入操作

**消息结构**：
每一条消息日志主要包含offset、MessageSize、data三个属性（还有一些其他属性）：
- **offset**：8字节，表示Message在这个partition中的全局偏移量，这是一个逻辑值而不是实际物理偏移量，它唯一确定了partition中的一条Message所在的逻辑位置，可以看作是partition中Message的id，offset从0开始
- **MessageSize**：4字节，表示消息内容的大小
- **data**：Message的具体内容，大小不固定

#### 偏移量索引文件（.index）

index文件作为偏移量索引文件，主要用于加快查找消息的速度。

**索引记录结构**：
该文件中的每一条索引记录都对应着log文件中的一条消息记录，一条索引记录包含**相对offset**和**position**两个属性（均为4字节）：

1. **相对offset**：
   - 因为数据文件分段以后，每个数据文件的起始offset不为0
   - 相对offset表示这条Message相对于其对应的数据文件中最小offset（也就是基准offset）的大小
   - 举例：分段后的一个数据文件的offset是从20开始，那么offset为25的Message在index文件中的相对offset就是25-20=5
   - 存储相对offset可以减小索引文件占用的空间

2. **position**：
   - 表示该条Message在数据文件中的物理位置
   - 只要打开对应log文件并移动文件指针到这个position就可以读取对应的Message内容了

**相对offset大小**：
- 相对offset大小为4字节，因此最大值为Integer.MAX_VALUE
- 在写入消息时会对要写入的相对offset进行校验
- 超过该值时将会自动进行日志段切分

**示例**：
```
假设某个LogSegment段中的数据文件名1234.log
索引文件中某个索引条目为（3,497）

解释：
- 这个索引条目对应的消息是数据文件中的第4个消息
- 该消息全局offset为1238（1234+3+1）
- 该消息的物理偏移地址（相对数据文件）为497
```

#### 稀疏索引

index索引文件中并没有为数据文件中的每条Message建立索引，而是采用了**稀疏索引**。

**特点**：
- 默认每间隔4k的数据建立一条索引
- 时间戳索引文件（.timeindex）也是这个规则
- 通过`log.index.interval.bytes`属性可以更新间隔数量大小

**优点**：
- 避免了索引文件占用过多的空间
- 可以将索引文件长期保留在内存中

**缺点**：
- 没有建立索引的Message不能一次性定位到其在数据文件的位置
- 需要做一次顺序扫描
- 但这次顺序扫描的范围很小

**某个index文件内容示例**：
```
offset:0 position:0
offset:20 position:320
offset:43 position:1220
```

**mmap技术**：
Kafka采用了mmap的方式直接将index文件映射到内存中（Java中就是MappedByteBuffer）。这样对index的操作就不需要操作磁盘IO，大大减少了磁盘IO次数和时间。

#### 时间戳索引文件（.timeindex）

和index偏移量索引文件一样，timeindex时间戳索引文件也是采用稀疏索引，默认每写入4k数据量的间隔记录一次时间索引项。

**索引结构**：
每个时间戳索引分为2部分，共占12个字节：
- **timestamp**：当前日志分段文件中建立索引的消息的时间戳（8字节）
- **relativeoffset**：时间戳对应消息的相对偏移量（4字节）

**某个timeindex文件内容示例**：
```
timestamp: 1627378362898 relativeoffset:0
timestamp: 1627378362998 relativeoffset:20
timestamp: 1627378363198 relativeoffset:43
```

### 3.3 查找消息

#### 根据offset查找消息

假设Kafka要根据offset来查找消息：

**步骤1：定位索引文件**
- 根据offset的值，查找对应的index索引文件
- 由于索引文件命名是以该文件的第一个绝对offset进行命名的
- 使用二分查找能够根据offset快速定位到对应的索引文件

**步骤2：定位消息位置**
- 找到index索引文件后，根据相对offset进行定位
- 找到索引文件中小于等于当前offset对应的相对offset的最大索引，并得到position（消息的物理偏移地址）
- 比如该offset的相对偏移量为100，而索引文件中存在的相对偏移量有（50,80,110）
- 那么最后找到的就是相对偏移量为80的消息的物理偏移地址

**步骤3：顺序扫描**
- 得到position以后，再到对应的log文件中，直接从对应position位置开始
- 查找offset对应的消息，向后依次顺序遍历
- 将每条消息的offset与目标offset进行比较，直到找到消息

**总结**：
- 由于index是稀疏索引，可能在索引文件中并不能直接定位到所要查找的消息的位置
- 还需要去log文件中做一次遍历查找
- 但是这次扫描的范围很小

#### 根据时间戳查找消息

如果根据时间戳来查找消息，需要横跨三个文件：**timeindex时间戳索引文件**、**index偏移量索引文件**、**log日志数据文件**。

因此相比于使用offset来查找消息要更慢一些。

**假设需要根据1627378362999这个时间戳来查询消息**：

**步骤1：定位日志分段**
- 将1627378362999和每个日志分段中最大时间戳largestTimeStamp逐一对比
- 直到找到一个不小于1627378362999的日志分段
- largestTimeStamp的计算：先查询该日志分段所对应时间戳索引文件，找到最后一条索引项
- 若最后一条索引项的时间戳字段值大于0，则取该值
- 否则去查找该日志分段的最近修改时间

**步骤2：二分查找时间戳索引**
- 找到相应日志分段之后，使用二分法进行定位
- 与偏移量索引方式类似，找到不大于1627378362999的最大索引项
- 也就是[1627378362998, 20]

**步骤3：查找偏移量索引**
- 拿着偏移量为20到偏移量索引文件中使用二分法找到不大于20的最大索引项
- 即[20, 320]

**步骤4：顺序扫描日志文件**
- 从log文件的position=320位置开始顺序扫描，找到时间戳为1627378362999的消息

### 3.4 Segment切分

Kafka会在以下情况下进行日志段切分：

1. **日志分段文件大小超过限制**：
   - 参数：`log.segment.bytes`
   - 默认值：1073741824（1GB）

2. **时间差超过限制**：
   - 参数：`log.roll.ms`或`log.roll.hours`（前者优先级高）
   - 默认：只配置了`log.roll.hours=168`（7天）

3. **索引文件大小超过限制**：
   - 参数：`log.index.size.max.bytes`
   - 默认值：10485760（10MB）
   - 偏移量索引文件或时间戳索引文件大小超过该值时切分

4. **相对位移量超过最大值**：
   - 新追加消息的offset与当前日志段的基础offset（baseOffset）差值大于Integer.MAX_VALUE时
   - 也就是相对位移量超过了最大值

### 3.5 Kafka为什么这么快（⭐⭐⭐⭐⭐核心）

我们都知道Kafka是基于磁盘进行存储的，但Kafka官方又称其具有高性能、高吞吐、低延迟的特点，其吞吐量动辄几十上百万。一般认为在磁盘上读写数据是会降低性能的，因为寻址会比较消耗时间，那Kafka又是怎么做到其吞吐量动辄几十上百万的呢？

**Kafka高性能，是多方面协同的结果**：
- 宏观架构
- 分布式partition存储
- ISR数据同步
- "无所不用其极"的高效利用磁盘、操作系统特性

#### 3.5.1 顺序写入（Sequential Write）

磁盘读写有两种方式：**顺序读写**或者**随机读写**。

**关键发现**：
- 在顺序读写的情况下，磁盘的顺序读写速度和内存持平
- 磁盘是机械结构，每次读写都会寻址→写入，其中寻址是一个"机械动作"
- 为了提高读写磁盘的速度，Kafka使用顺序I/O

**性能对比**：
```
随机I/O vs 顺序I/O 性能差距：3-4个数量级

7200转/分钟的SATA机械硬盘：
- 随机I/O：~100 IOPS
- 顺序I/O：~100 MB/s

即使在SSD上，顺序I/O和随机I/O之间的性能差距仍然很明显
```

**Kafka的设计**：
- Kafka利用了一种**分段式的、只追加(Append-Only)的日志**
- 基本上把自身的读写操作限制为顺序I/O
- 使得它在各种存储介质上能有很快的速度

**写入过程**：
```
每一个Partition其实都是一个文件

Producer发送消息 → Kafka把数据插入到文件末尾（虚框部分）

Partition 0:
[msg1][msg2][msg3][msg4][msg5]...[msgN] ← 新消息追加到这里
```

**只读设计**：
- Kafka不会修改、删除数据
- 把所有的数据都保留下来
- 每个消费者（Consumer）对每个Topic都有一个offset用来表示读取到了第几条数据

**操作系统优化**：
- 磁盘的顺序读写是磁盘使用模式中最有规律的
- 操作系统对这种模式做了大量优化
- Kafka的message是不断追加到本地磁盘文件末尾的，而不是随机的写入
- 这使得Kafka写入吞吐量得到了显著提升

#### 3.5.2 页缓存（Page Cache）

即便是顺序写入硬盘，硬盘的访问速度还是不可能追上内存。所以**Kafka的数据并不是实时的写入硬盘**，它充分利用了现代操作系统分页存储来利用内存提高I/O效率。

**核心思想**：
- 把磁盘中的数据缓存到内存中
- 把对磁盘的访问变为对内存的访问

#### Memory Mapped Files（mmap）

简称`mmap`，简单描述其作用就是：**将磁盘文件映射到内存，用户通过修改内存就能修改磁盘文件**。

**工作原理**：
- 直接利用操作系统的Page来实现磁盘文件到物理内存的直接映射
- 完成映射之后，对物理内存的操作会被同步到硬盘上（操作系统在适当的时候）

**传统方式 vs mmap**：
```
传统方式：
磁盘 → 内核空间 → 用户空间 → 内核空间 → 网卡（4次拷贝）

mmap方式：
磁盘 → 内核空间（Page Cache）→ 用户空间直接操作
用户空间修改 → 内核空间 → 磁盘（操作系统控制）
```

**优点**：
- 进程像读写硬盘一样读写内存（虚拟内存）
- 可以获取很大的I/O提升
- 省去了用户空间到内核空间复制的开销

**缺点**：
- 不可靠，写到mmap中的数据并没有被真正的写到硬盘
- 操作系统会在程序主动调用flush的时候才把数据真正的写到硬盘

**Kafka的控制**：
Kafka提供了一个参数`producer.type`来控制是不是主动flush：
- **同步（sync）**：Kafka写入到mmap之后就立即flush，然后再返回Producer
- **异步（async）**：写入mmap之后立即返回Producer不调用flush

#### Java NIO对文件映射的支持

Java NIO提供了一个**MappedByteBuffer**类可以用来实现内存映射。

**使用方式**：
- MappedByteBuffer只能通过调用FileChannel的map()取得，再没有其他方式
- FileChannel.map()是抽象方法，具体实现是在FileChannelImpl.map()
- 其map0()方法就是调用了Linux内核的mmap的API

**注意事项**：
- mmap的文件映射，在full gc时才会进行释放
- 当close时，需要手动清除内存映射文件，可以反射调用sun.misc.Cleaner方法

#### Page Cache工作机制

**当一个进程准备读取磁盘上的文件内容时**：
1. 操作系统会先查看待读取的数据所在的页（page）是否在页缓存（pagecache）中
2. 如果存在（命中）则直接返回数据，从而避免了对物理磁盘的I/O操作
3. 如果没有命中，则操作系统会向磁盘发起读取请求并将读取的数据页存入页缓存，之后再将数据返回给进程

**当一个进程需要将数据写入磁盘时**：
1. 操作系统也会检测数据对应的页是否在页缓存中
2. 如果不存在，则会先在页缓存中添加相应的页，最后将数据写入对应的页
3. 被修改过后的页也就变成了脏页，操作系统会在合适的时间把脏页中的数据写入磁盘，以保持数据的一致性

**Kafka的优势**：
- 对一个进程而言，它会在进程内部缓存处理所需的数据
- 然而这些数据有可能还缓存在操作系统的页缓存中
- 因此同一份数据有可能被缓存了两次
- 当使用页缓存的时候，即使Kafka服务重启，页缓存还是会保持有效
- 然而进程内的缓存却需要重建
- 这样也极大地简化了代码逻辑，因为维护页缓存和文件之间的一致性交由操作系统来负责
- Kafka中大量使用了页缓存，这是Kafka实现高吞吐的重要因素之一
- 消息先被写入页缓存，由操作系统负责刷盘任务

#### 3.5.3 零拷贝（Zero-Copy）⭐⭐⭐⭐⭐

导致应用程序效率低下的一个典型根源是**缓冲区之间的字节数据拷贝**。

**Kafka的解决方案**：
- Kafka使用由Producer、Broker和Consumer多方共享的二进制消息格式
- 数据块即便是处于压缩状态也可以在不被修改的情况下在端到端之间流动
- 虽然消除通信各方之间的结构化差异是非常重要的一步，但它本身并不能避免数据的拷贝
- Kafka通过利用Java的NIO框架，尤其是`java.nio.channels.FileChannel`里的`transferTo`这个方法，解决了数据拷贝问题

#### 传统方式的数据拷贝

**传统方式代码**：
```java
File.read(fileDesc, buf, len);
Socket.send(socket, buf, len);
```

**传统方式流程**：
```
1. read()调用 → 用户态切换到内核态
2. DMA引擎读取文件 → 拷贝到内核缓冲区
3. 内核缓冲区 → 拷贝到用户空间缓冲区 → 切换回用户态
4. send()调用 → 用户态切换到内核态
5. 用户空间缓冲区 → 拷贝到内核Socket缓冲区
6. Socket缓冲区 → DMA拷贝到NIC缓冲区 → 网络传输
7. send()返回 → 切换回用户态

总计：4次拷贝 + 4次上下文切换
```

**详细步骤**：
1. 初始的read()调用导致了一次用户态到内核态的上下文切换。DMA引擎读取文件，并将其内容复制到内核地址空间中的缓冲区中
2. 在从read()返回之前，内核缓冲区的数据会被拷贝到用户态的缓冲区。此时，程序可以读取文件的内容
3. 接下来的send()方法会切换回内核态，拷贝用户态的缓冲区数据到内核地址空间——这一次是拷贝到一个关联着目标套接字的不同缓冲区。在后台，DMA引擎会接手这一操作，异步地把数据从内核缓冲区拷贝到协议堆栈，由网卡进行网络传输
4. send()调用返回，切换回用户态

**问题**：
- 尽管模式切换的效率很低，而且需要进行额外的拷贝
- 当请求的数据量极大地超过内核缓冲区大小时，内核缓冲区会成为性能瓶颈
- 它不会直接拷贝数据，而是迫使系统在用户态和内核态之间摇摆，直到所有数据都被传输完成

#### 零拷贝方式

**零拷贝代码**：
```java
fileDesc.transferTo(offset, len, socket);
```

**零拷贝流程（第一阶段优化）**：
```
1. transferTo()调用 → 用户态切换到内核态
2. DMA引擎读取文件 → 拷贝到内核读缓冲区
3. 内核读缓冲区 → 拷贝到Socket缓冲区
4. Socket缓冲区 → DMA拷贝到NIC缓冲区 → 网络传输
5. transferTo()返回 → 切换回用户态

总计：3次拷贝 + 2次上下文切换（相比传统方式）
```

**详细步骤**：
1. transferTo()方法指示数据块设备通过DMA引擎将数据读入读缓冲区
2. 然后这个缓冲区的数据拷贝到另一个内核缓冲区中，分阶段写入套接字
3. 最后，DMA将套接字缓冲区的数据拷贝到NIC缓冲区中

**结果**：
- 拷贝的次数从4降到了3
- 其中只有一次拷贝占用了CPU资源
- 上下文切换的次数从4降到了2

#### 真正的零拷贝（gather操作）

通过利用Linux内核2.4或更高版本以及支持gather操作的网卡，可以实现真正的"零拷贝"。

**真正零拷贝流程**：
```
1. transferTo()调用 → 用户态切换到内核态
2. DMA引擎读取文件 → 拷贝到内核读缓冲区
3. 读缓冲区的指针（包含偏移量和长度）→ 传递给Socket缓冲区
4. NIC根据指针直接从读缓冲区通过DMA拷贝数据到NIC缓冲区
5. transferTo()返回 → 切换回用户态

总计：2次拷贝（都是DMA）+ 2次上下文切换
```

**详细步骤**：
1. 调用transferTo()方法会致使设备通过DMA引擎将数据读入内核读缓冲区
2. 通过gather操作，读缓冲区和套接字缓冲区之间的数据拷贝将不复存在
3. 相反地，NIC被赋予一个指向读缓冲区的指针，连同偏移量和长度
4. 所有数据都将通过DMA抽取干净并拷贝到NIC缓冲区
5. 在这个过程中，在缓冲区间拷贝数据将无需占用任何CPU资源

**性能提升**：
- 传统的方式和零拷贝方式在MB字节到GB字节的文件大小范围内的性能对比显示
- 零拷贝方式相较于传统方式的性能提升幅度在2到3倍
- 更令人惊叹的是，Kafka仅仅是在一个纯JVM虚拟机下、没有使用本地库或JNI代码，就实现了这一点

**Kafka的实现**：
- 把磁盘文件读取OS内核缓冲区后的fileChannel，直接转给socketChannel发送
- 底层就是sendfile
- 消费者从broker读取数据，就是由此实现
- Kafka的数据传输通过TransportLayer来完成
- 其子类PlaintextTransportLayer通过Java NIO的FileChannel的transferTo和transferFrom方法实现零拷贝

#### 3.5.4 批量处理与压缩

##### 日志记录批处理

顺序I/O在大多数的存储介质上都非常快，几乎可以和网络I/O的峰值性能相媲美。

**特点**：
- 在实践中，这意味着一个设计良好的日志结构的持久层将可以紧随网络流量的速度
- 事实上，Kafka的瓶颈通常是网络而非磁盘
- 除了由操作系统提供的底层批处理能力之外，Kafka的Clients和Brokers会把多条读写的日志记录合并成一个批次，然后才通过网络发送出去
- 日志记录的批处理通过使用更大的包以及提高带宽效率来摊薄网络往返的开销

##### 批量压缩

当启用压缩功能时，批处理的影响尤为明显，因为压缩效率通常会随着数据量大小的增加而变得更高。

**特点**：
- 特别是当使用JSON等基于文本的数据格式时，压缩效果会非常显著
- 压缩比通常能达到5到7倍
- 日志记录批处理在很大程度上是作为Client侧的操作完成的
- 此举把负载转移到Client上
- 不仅对网络带宽效率、而且对Brokers的磁盘I/O利用率也有很大的提升

##### 非强制刷新缓冲写操作

另一个助力Kafka高性能、同时也是一个值得更进一步去探究的底层原因：**Kafka在确认写成功ACK之前的磁盘写操作不会真正调用fsync命令**。

**特点**：
- 通常只需要确保日志记录被写入到I/O Buffer里就可以给Client回复ACK信号
- 这是一个鲜为人知却至关重要的事实
- 事实上，这正是让Kafka能表现得如同一个内存型消息队列的原因
- Kafka是一个基于磁盘的内存型消息队列（受缓冲区/页面缓存大小的限制）

**风险**：
- 这种形式的写入是不安全的
- 因为副本的写失败可能会导致数据丢失
- 即使日志记录似乎已经被确认成功
- 与关系型数据库不同，确认一个写操作成功并不等同于持久化成功

**解决方案**：
- 真正使得Kafka具备持久化能力的是运行多个同步的副本的设计
- 即便有一个副本写失败了，其他的副本（假设有多个）仍然可以保持可用状态
- 前提是写失败是不相关的（例如，多个副本由于一个共同的上游故障而同时写失败）
- 因此，不使用fsync的I/O非阻塞方法和冗余同步副本的结合
- 使得Kafka同时具备了高吞吐量、持久性和可用性

#### 3.5.5 流数据并行

日志结构I/O的效率是影响性能的一个关键因素，主要影响写操作。Kafka在对Topic结构和Consumer群组的并行处理是其读性能的基础。

**分区机制的优势**：
- 分区机制使得Kafka Brokers可以水平扩展
- 每个分区都有一个专门的Leader
- 因此，任何重要的主题Topic（具有多个分区）都可以利用整个Broker集群进行写操作
- 这是Kafka和消息队列之间的另一个区别
- 后者利用集群来获得可用性，而Kafka将真正地在Brokers之间负载均衡，以获得可用性、持久性和吞吐量

**分区分配**：
- 生产者在发布日志记录之时指定分区
- 假设你正在发布消息到一个有多个分区的Topic上
- 这可以通过直接指定分区索引来完成
- 或者间接通过日志记录的键值来完成，该键值能被确定性地哈希到一个一致的分区索引
- 拥有相同哈希值的日志记录将会被存储到同一个分区中

**消费者并行**：
- 日志记录的实际处理是由一个在（可选的）Consumer Group中的Consumer操作完成
- Kafka确保一个分区最多只能分配给它的Consumer Group中的一个Consumer
- 当第一个Consumer Group里的Consumer订阅了Topic，它将消费这个Topic下的所有分区的数据
- 当第二个Consumer紧随其后加入订阅时，它将大致获得这个Topic的一半分区，减轻第一个Consumer先前负荷的一半
- 这使得你能够并行处理事件流，并根据需要增加Consumer（理想情况下，使用自动伸缩机制）

**日志记录吞吐量的控制**：
1. **Topic的分区方案**：
   - 应该对Topics进行分区，以最大限度地增加独立子事件流的数量
   - 日志记录的顺序应该只保留在绝对必要的地方
   - 如果任意两个日志记录在某种意义上没有合理的关联，那它们就不应该被绑定到同一个分区
   - 这暗示你要使用不同的键值，因为Kafka将使用日志记录的键值作为一个散列源来派生其一致的分区映射

2. **一个组里的Consumers数量**：
   - 可以增加Consumer Group里的Consumer数量来均衡入站的日志记录的负载
   - 这个数量的上限是Topic的分区数量
   - 如果你愿意的话，当然可以增加更多的Consumers
   - 不过分区计数将会设置一个上限来确保每一个活跃的Consumer至少被指派到一个分区
   - 多出来的Consumers将会一直保持在一个空闲的状态
   - Consumer可以是进程或线程
   - 依据Consumer执行的工作负载类型，可以在线程池中使用多个独立的Consumer线程或进程记录

---

## 四、水印备份机制（⭐⭐⭐⭐⭐核心）

### 4.1 概述

高可用是很多分布式系统中必备的特征之一，Kafka日志的高可用是通过基于leader-follower的多副本同步实现的。

**问题**：
- 每个分区下有多个副本，其中只有一个是leader副本，提供发送和消费消息
- 其余都是follower副本，不断地发送fetch请求给leader副本以同步消息
- 如果leader在整个集群运行过程中不发生故障，follower副本不会起到任何作用
- 问题在于任何系统都不能保证其稳定运行
- 当leader副本所在的broker崩溃之后，其中一个follower副本就会成为该分区下新的leader副本

**核心问题**：
- 在选为新的leader副本时，会导致消息丢失或离散吗？
- Kafka是如何解决leader副本变更时消息不会出错？
- leader与follower副本之间的数据同步是如何进行的？

### 4.2 水印相关概念

在讲解水印备份之前，我们必须要先搞清楚几个关键的术语以及它们的含义。

**示意图**：
```
Partition Replica (Leader)
┌─────────────────────────────────┐
│  offset: 0   1   2   3   4   5  │
│  msg:   [A] [B] [C] [D] [E] [F] │
│              ↑               ↑   │
│             HW              LEO  │
└─────────────────────────────────┘

绿色部分（0-2）：已完全备份的消息，对消费者可见
紫色部分（3-5）：未完全备份的消息，对消费者不可见
```

#### LEO（Last End Offset）- 日志末端位移

**定义**：
- 记录了该副本对象底层日志文件中下一条消息的位移值
- 副本写入消息的时候，会自动更新LEO值

**特点**：
- LEO指向的是下一条消息的位置，而不是当前最新消息的位置
- 如果当前副本有6条消息（offset 0-5），则LEO=6

#### HW（High Watermark）- 高水印

**定义**：
- 从名字可以知道，该值叫高水印值
- HW一定不会大于LEO值
- 小于HW值的消息被认为是"已提交"或"已备份"的消息，并对消费者可见

**作用**：
1. **定义消息可见性**：
   - 用来标识分区下的哪些消息是可以被消费者消费的
   - 只有在高水位以下的消息才能被消费者进行消费
   - 高水位之前的消息才被认为是已提交的消息，才可以被消费

2. **完成Kafka副本的消息同步**：
   - 利用HW机制来完成kafka副本的消息同步

**示例**：
```
比如某个分区的HW（leader的HW）是8
那么这个分区只有 < 8 这些位置上消息可以被消费
即offset 0-7的消息可以被消费，offset 8及以后的消息不可见
```

#### Remote LEO

**定义**：
- leader会保存两个类型的LEO值
- 一个是自己的LEO
- 另一个是remote LEO值，remote LEO值就是follower副本的LEO值
- 意味着follower副本的LEO值会保存两份：一份保存到leader副本中，一份保存到自己这里

**作用**：
- 它是决定HW值大小的关键
- 当HW要更新时，就会对比LEO值（也包括leader LEO）
- 取最小的那个做最新的HW值

### 4.3 水印更新机制

#### LEO更新

**1. Leader副本自身的LEO值更新**：
- 在Producer消息发送过来时更新
- 即leader副本当前最新存储的消息位移位置+1

**2. Follower副本自身的LEO值更新**：
- 从leader副本中fetch到消息并写到本地日志文件时更新
- 即follower副本当前同步leader副本最新的消息位移位置+1

**3. Leader副本中的Remote LEO值更新**：
- 每次follower副本发送fetch请求都会包含follower当前LEO值
- leader拿到该值就会尝试更新remote LEO值

#### Leader HW更新

##### 故障时更新

**1. 副本被选为leader副本时**：
- 当某个follower副本被选为分区的leader副本时
- Kafka就会尝试更新HW值

**2. 副本被踢出ISR时**：
- 如果某个副本追不上leader副本进度，或者所在broker崩溃了
- 导致被踢出ISR，leader也会检查HW值是否需要更新
- 毕竟HW值更新只跟处于ISR的副本LEO有关系

##### 正常时更新

**1. Producer向leader副本写入消息时**：
- 在消息写入时会更新leader LEO值
- 因此需要再检查是否需要更新HW值

**2. Leader处理follower FETCH请求时**：
- follower的fetch请求会携带LEO值
- leader会根据这个值更新对应的remote LEO值
- 同时也需要检查是否需要更新HW值

#### Follower HW更新

follower更新HW发生在其更新LEO之后：
- 每次follower Fetch响应体都会包含leader的HW值
- 然后比较当前LEO值，取最小的作为新的HW值

### 4.4 备份过程

假设某个分区有两个副本，`min.insync.replica=1`：

#### Step 1：初始化状态

```
Leader Replica          Follower Replica
┌──────────────┐        ┌──────────────┐
│ LEO: 0       │        │ LEO: 0       │
│ HW:  0       │        │ HW:  0       │
│ Remote LEO:0 │        │              │
└──────────────┘        └──────────────┘
        ↑                       ↓
        └───── Fetch Request ───┘
        (offset=0, 无数据返回)
```

- Leader和follower副本处于初始化值
- Follower副本发送fetch请求
- 由于leader副本没有数据，因此不会进行同步操作

#### Step 2：生产者发送消息

```
Producer发送消息m1
        ↓
Leader Replica          Follower Replica
┌──────────────┐        ┌──────────────┐
│ [m1]         │        │              │
│ LEO: 1       │        │ LEO: 0       │
│ HW:  0       │        │ HW:  0       │
│ Remote LEO:0 │        │              │
└──────────────┘        └──────────────┘
```

- 生产者发送了消息m1到分区leader副本
- 写入该条消息后leader更新LEO=1
- HW还未更新，仍为0

#### Step 3：第一轮Fetch请求

```
Follower发送Fetch请求(offset=0)
        ↓
Leader处理：
1. 更新Remote LEO=0
2. 对比LEO值：min(Leader LEO=1, Remote LEO=0) = 0
3. 更新Leader HW=0
4. 响应：消息m1 + Leader HW=0

Leader Replica          Follower Replica
┌──────────────┐        ┌──────────────┐
│ [m1]         │        │ [m1]         │
│ LEO: 1       │        │ LEO: 1       │
│ HW:  0       │        │ HW:  0       │
│ Remote LEO:0 │        │              │
└──────────────┘        └──────────────┘
```

- Follower发送fetch请求，携带当前最新的offset=0
- Leader处理fetch请求时，更新remote LEO=0
- 对比LEO值最小为0，所以HW=0
- Leader副本响应消息数据及leader HW=0给follower
- Follower写入消息后，更新LEO值
- 同时对比leader HW值，取最小的作为新的HW值
- 此时follower HW=0
- **这也意味着，follower HW是不会超过leader HW值的**

#### Step 4：第二轮Fetch请求

```
Follower发送Fetch请求(offset=1)
        ↓
Leader处理：
1. 更新Remote LEO=1
2. 对比LEO值：min(Leader LEO=1, Remote LEO=1) = 1
3. 更新Leader HW=1
4. 响应：无新消息 + Leader HW=1

Leader Replica          Follower Replica
┌──────────────┐        ┌──────────────┐
│ [m1]         │        │ [m1]         │
│ LEO: 1       │        │ LEO: 1       │
│ HW:  1       │        │ HW:  1       │
│ Remote LEO:1 │        │              │
└──────────────┘        └──────────────┘
```

- Follower发送第二轮fetch请求，携带当前最新的offset=1
- Leader处理fetch请求时，更新remote LEO=1
- 对比LEO值最小为1，所以HW=1
- 此时leader没有新的消息数据，所以直接返回leader HW=1给follower
- Follower对比当前最新的LEO值与leader HW值，取最小的作为新的HW值
- 此时follower HW=1

**关键点**：
- Leader中保存的remote LEO值的更新总是需要额外一轮fetch RPC请求才能完成
- 这意味着在leader切换过程中，会存在数据丢失以及数据不一致的问题

### 4.5 水印机制的缺陷

#### 数据丢失场景

```
场景：副本A(Follower)、副本B(Leader)

Step 1: B已将HW更新为2
┌─────────────┐        ┌─────────────┐
│ A(Follower) │        │ B(Leader)   │
│ [m0][m1]    │        │ [m0][m1]    │
│ LEO: 2      │        │ LEO: 2      │
│ HW:  1      │        │ HW:  2      │
└─────────────┘        └─────────────┘
     ↓ A崩溃重启
     
Step 2: A重启后日志截断(HW=1)
┌─────────────┐        ┌─────────────┐
│ A(Follower) │        │ B(Leader)   │
│ [m0]        │        │ [m0][m1]    │
│ LEO: 1      │        │ LEO: 2      │
│ HW:  1      │        │ HW:  2      │
└─────────────┘        └─────────────┘
     ↑                      ↓ B崩溃
     
Step 3: A被选为新Leader
┌─────────────┐        ┌─────────────┐
│ A(Leader)   │        │ B(Follower) │
│ [m0]        │        │ [m0]        │
│ LEO: 1      │        │ LEO: 1      │
│ HW:  1      │        │ HW:  1      │
└─────────────┘        └─────────────┘

结果：offset=1的消息永久丢失！
```

**原因分析**：
1. A在第二轮fetch请求并接收到响应之后，B已经将HW更新为2
2. 如果这时A还没处理完响应就崩溃了，即follower没有及时更新HW值
3. A重启时，会自动将LEO值调整到之前的HW值，即会进行日志截断
4. 接着A会向B发送fetch请求，但很不幸的是此时B也发生宕机了
5. Kafka会将A选举为新的分区Leader
6. 当B重启后，会从A发送fetch请求，收到fetch响应后，拿到HW值，并更新本地HW值
7. 此时HW被调整为1（之前是2），这时B会做日志截断
8. 因此，offset=1的消息被永久地删除了

**为什么Follower要进行日志截断？**
- 消息会先记录到leader，follower再从leader中拉取消息进行同步
- 这就导致leader LEO会比follower的要大
- 假设此时出现leader切换，有可能选举了一个LEO较小的follower成为新的leader
- 这时该副本的LEO就会成为新的标准
- 这就会导致follower LEO值有可能会比leader LEO值要大的情况
- 因此follower在进行同步之前，需要从leader获取LastOffset的值
- 如果LastOffset小于当前LEO，则需要进行日志截断，然后再从leader拉取数据实现同步

**日志截断会不会造成数据丢失？**
- HW值以上的消息是没有"已提交"或"已备份"的
- 因此消息也是对消费者不可见
- 即这些消息不对用户作承诺
- 也即是说从HW值截断日志，并不会导致数据丢失（承诺用户范围内）

#### 数据不一致/离散场景

```
场景：副本A(Leader)、副本B(Follower)

条件：
1. B宕机之前已不在ISR列表中，unclean.leader.election.enable=true
   或
2. B消息写入到pagecache，但尚未flush到磁盘

Step 1: 初始状态
┌─────────────┐        ┌─────────────┐
│ A(Leader)   │        │ B(Follower) │
│ [m0][m1]    │        │ [m0]        │
│ LEO: 2      │        │ LEO: 1      │
│ HW:  2      │        │ HW:  1      │
└─────────────┘        └─────────────┘
     ↓ A和B同时宕机
     
Step 2: B先重启成为Leader
┌─────────────┐        ┌─────────────┐
│ A(停机)     │        │ B(Leader)   │
│             │        │ [m0][m2]    │
│             │        │ LEO: 2      │
│             │        │ HW:  2      │
└─────────────┘        └─────────────┘
                    ↑ 生产者发送新消息m2
                    
Step 3: A重启
┌─────────────┐        ┌─────────────┐
│ A(Follower) │        │ B(Leader)   │
│ [m0][m1]    │        │ [m0][m2]    │
│ LEO: 2      │        │ LEO: 2      │
│ HW:  2      │        │ HW:  2      │
└─────────────┘        └─────────────┘

结果：A的offset=1是m1，B的offset=1是m2，数据不一致！
```

**原因分析**：
1. A已经写入两条消息，且HW更新到2
2. B只写了1条消息，HW为1
3. A和B同时宕机，B先重启，B成为了leader副本
4. 这时生产者发送了一条消息m2，保存到B中
5. 由于此时分区只有B，B在写入消息时把HW更新到2
6. 就在这时候A重新启动，发现leader HW为2，跟自己的HW一样
7. 因此没有执行日志截断
8. 这就造成了A的offset=1的日志与B的offset=1的日志不一样的现象

### 4.6 Leader Epoch机制

为了解决HW更新时机是异步延迟的，而HW又是决定日志是否备份成功的标志，从而造成数据丢失和数据不一致的现象，**Kafka引入了leader epoch机制**。

#### Leader Epoch是什么

在每个副本日志目录下都创建一个`leader-epoch-checkpoint`文件，用于保存leader的epoch信息。

**格式**：`(epoch, offset)`
- **epoch**：leader版本，它是一个单调递增的一个正整数值，每次leader变更，epoch版本都会+1
- **offset**：每一代leader写入的第一条消息的位移值

**示例**：
```
(0, 0)
(1, 300)
(2, 650)
```

解释：
- 第0代leader从位移0开始写入消息
- 第1代leader从位移300开始写入消息（意味着第0代写入了0-299的消息）
- 第2代leader从位移650开始写入消息（意味着第1代写入了300-649的消息）

#### Leader Epoch工作机制

##### 当副本成为Leader时

如果此时生产者有新消息发送过来，会首先将新的leader epoch以及LEO添加到`leader-epoch-checkpoint`文件中。

##### 当副本变成Follower时

**1. 发送LeaderEpochRequest请求给leader副本**：
- 该请求包括了follower中最新的epoch版本

**2. Leader返回LastOffset**：
- 如果`follower last epoch = leader last epoch`，则`LastOffset = leader LEO`
- 否则取大于follower last epoch中最小的leader epoch的start offset值
- 举例：假设follower last epoch=1，此时leader有(1,20) (2,80) (3,120)，则LastOffset=80

**3. Follower进行日志截断判断**：
- Follower拿到LastOffset之后，会对比当前LEO值是否大于LastOffset
- 如果当前LEO大于LastOffset，则从LastOffset截断日志

**4. Follower开始同步**：
- Follower开始发送fetch请求给leader保持消息同步

#### 解决数据丢失

```
场景：副本A(Follower)、副本B(Leader)

Step 1: 初始状态
┌─────────────────┐        ┌─────────────────┐
│ A(Follower)     │        │ B(Leader)       │
│ [m0][m1]        │        │ [m0][m1]        │
│ LEO: 2          │        │ LEO: 2          │
│ HW:  1          │        │ HW:  2          │
│ Epoch: (0, 0)   │        │ Epoch: (0, 0)   │
└─────────────────┘        └─────────────────┘
     ↓ A崩溃重启
     
Step 2: A重启发送LeaderEpochRequest
┌─────────────────┐        ┌─────────────────┐
│ A(Follower)     │        │ B(Leader)       │
│ Request:        │───────>│ Response:       │
│ epoch=0         │        │ LastOffset=2    │
│                 │<───────│ (epoch=0相同)   │
└─────────────────┘        └─────────────────┘

Step 3: A判断不需要截断
┌─────────────────┐        ┌─────────────────┐
│ A(Follower)     │        │ B(Leader)       │
│ [m0][m1]        │        │ [m0][m1]        │
│ LEO: 2          │        │ LEO: 2          │
│ LastOffset=2    │        │ HW:  2          │
│ 不需要截断!     │        │                 │
└─────────────────┘        └─────────────────┘
     ↑                          ↓ B崩溃
     
Step 4: A成为Leader，B重启后同步
┌─────────────────┐        ┌─────────────────┐
│ A(Leader)       │        │ B(Follower)     │
│ [m0][m1]        │        │ [m0][m1]        │
│ LEO: 2          │        │ LEO: 2          │
│ Epoch: (1, 2)   │        │ 同样不需要截断  │
└─────────────────┘        └─────────────────┘

结果：数据没有丢失！
```

**关键点**：
- A重启之后，发送LeaderEpochRequest请求给B
- 由于B还没追加消息，此时epoch=request epoch=0
- 因此返回LastOffset=leader LEO=2给A
- A拿到LastOffset之后，发现等于当前LEO值，故不用进行日志截断
- 就在这时B宕机了，A成为leader
- 在B启动回来后，会重复A的动作，同样不需要进行日志截断
- 数据没有丢失

#### 解决数据不一致/离散

```
场景：副本A(Leader)、副本B(Follower)

Step 1: A和B同时宕机
┌─────────────────┐        ┌─────────────────┐
│ A(停机)         │        │ B(停机)         │
│ [m0][m1]        │        │ [m0]            │
│ Epoch: (0, 0)   │        │ Epoch: (0, 0)   │
└─────────────────┘        └─────────────────┘
     
Step 2: B先重启成为Leader，接收新消息
┌─────────────────┐        ┌─────────────────┐
│ A(停机)         │        │ B(Leader)       │
│                 │        │ [m0][m2]        │
│                 │        │ LEO: 2          │
│                 │        │ Epoch: (1, 1)   │
└─────────────────┘        └─────────────────┘
                        ↑ 生产者发送消息m2
                        ↑ Epoch更新为1
                        
Step 3: A重启发送LeaderEpochRequest
┌─────────────────┐        ┌─────────────────┐
│ A(Follower)     │        │ B(Leader)       │
│ Request:        │───────>│ Response:       │
│ epoch=0         │        │ LastOffset=1    │
│                 │<───────│ (找到epoch=1)   │
└─────────────────┘        └─────────────────┘

Step 4: A进行日志截断
┌─────────────────┐        ┌─────────────────┐
│ A(Follower)     │        │ B(Leader)       │
│ [m0]            │        │ [m0][m2]        │
│ LEO: 1          │        │ LEO: 2          │
│ 截断offset=1!   │        │ Epoch: (1, 1)   │
└─────────────────┘        └─────────────────┘
     ↓ 开始同步
     
Step 5: A同步完成
┌─────────────────┐        ┌─────────────────┐
│ A(Follower)     │        │ B(Leader)       │
│ [m0][m2]        │        │ [m0][m2]        │
│ LEO: 2          │        │ LEO: 2          │
│ Epoch: (1, 1)   │        │ Epoch: (1, 1)   │
└─────────────────┘        └─────────────────┘

结果：数据一致，避免了离散问题！
```

**关键点**：
- A和B同时宕机后，B先重启回来成为分区leader
- 这时候生产者发送了一条消息过来，leader epoch更新到1
- 此时A启动回来后，发送LeaderEpochRequest（follower epoch=0）给B
- B判断follower epoch不等于最新的epoch
- 于是找到大于follower epoch最小的epoch=1，即LastOffset=epoch start offset=1
- A拿到LastOffset后，判断小于当前LEO值，于是从LastOffset位置进行日志截断
- 接着开始发送fetch请求给B开始同步消息
- 避免了消息不一致/离散的问题

### 4.7 副本同步机制

**ISR（In-Sync Replica）**就是Kafka为某个分区维护的一组同步集合。

**特点**：
- 每个分区都有自己的一个ISR集合
- 处于ISR集合中的副本，意味着follower副本与leader副本保持同步状态
- 只有处于ISR集合中的副本才有资格被选举为leader
- 一条Kafka消息，只有被ISR中的副本都接收到，才被视为"已同步"状态
- 这跟ZK的同步机制不一样，ZK只需要超过半数节点写入，就可被视为已写入成功

#### 副本同步流程

```
Follower副本与Leader副本之间的数据同步流程：

Leader Replica                    Follower Replica
┌──────────────┐                  ┌──────────────┐
│ [m0][m1][m2] │                  │ [m0][m1]     │
│ LEO: 3       │                  │ LEO: 2       │
│ HW:  2       │                  │ HW:  1       │
│ Remote LEO:1 │                  │              │
└──────────────┘                  └──────────────┘
        ↑                                 ↓
        └──── Fetch Request(offset=2) ────┘
        
        ↓ Leader处理
        
┌──────────────┐                  ┌──────────────┐
│ [m0][m1][m2] │                  │ [m0][m1][m2] │
│ LEO: 3       │                  │ LEO: 3       │
│ HW:  2       │                  │ HW:  2       │
│ Remote LEO:2 │                  │              │
└──────────────┘                  └──────────────┘
        ↑                                 ↓
        └──── Response(m2, HW=2) ─────────┘
```

**关键点**：
- Leader的remote LEO的值相对于follower LEO值，滞后一个follower RPC请求
- Remote LEO决定leader HW值的大小
- Leader副本永远领先follower副本
- 各个follower副本之间的消息最新位移也不尽相同

#### 0.9.0.0版本之前的设计

**参数**：`replica.lag.max.messages`
- 含义：允许follower副本落后leader副本的消息数量
- 超过这个数量后，follower会被踢出ISR

**问题**：
- 很难在生产上给出一个合理值
- 如果给的小，会导致follower频繁被踢出ISR
- 如果给的大，broker发生宕机导致leader变更时，可能会发生日志截断，导致消息严重丢失

**为什么难以设置适中值？**

假设某个Kafka集群追求高吞吐量：
- 生产者的`batch.size`会设置得很大，每次发送包含的消息量很多
- 如果此时`min.insync.replicas=1`
- 生产者发送消息保存到leader副本后就会响应成功
- 由于follower副本同步leader副本的消息是不断地发送fetch请求
- 此时如果leader一下子接收到很多消息
- 就会导致leader副本与follower副本的消息数量相差很大
- 如果此时这个差数大于`replica.lag.max.messages`的值
- Follower副本就会被踢出ISR
- 因此，该集群需要把`replica.lag.max.messages`的值设置成很大才能够避免follower副本频繁被踢出ISR
- 但这样会造成消息严重丢失的风险

#### 0.9.0.0版本之后的设计

**参数**：`replica.lag.time.max.ms`
- 默认值：10000（10秒）
- 含义：允许follower副本不同步消息的最大时间值
- 判断：只要在`replica.lag.time.max.ms`时间内follower有同步消息，即认为该follower处于ISR中

**优点**：
- 很好地避免了在某个瞬间生产者一下子发送大量消息到leader副本
- 导致该分区ISR频繁收缩与扩张的问题

## 五、生产者原理

### 5.1 消息缓冲池（RecordAccumulator）

#### 为什么需要消息缓冲池？

Kafka为了提高Producer客户端的发送吞吐量和提高性能，选择了将消息暂时缓存起来，等到满足一定的条件，再进行批量发送。这样可以减少网络请求，提高吞吐量。

**核心类**：`RecordAccumulator`

#### 消息缓存模型

```
RecordAccumulator（消息累加器）
│
├─ TopicPartition 0
│  └─ Deque<ProducerBatch>
│     ├─ ProducerBatch 1 (16KB, 已满)
│     ├─ ProducerBatch 2 (16KB, 已满)
│     └─ ProducerBatch 3 (8KB, 未满) ← 当前写入
│
├─ TopicPartition 1
│  └─ Deque<ProducerBatch>
│     └─ ProducerBatch 1 (5KB, 未满)
│
└─ TopicPartition 2
   └─ Deque<ProducerBatch>
      └─ ProducerBatch 1 (12KB, 未满)

触发发送条件：
1. batch.size满了（默认16KB）
2. linger.ms超时（默认0ms）
```

**工作流程**：
1. 每条消息，按照TopicPartition维度，放在不同的`Deque<ProducerBatch>`队列里面
2. TopicPartition相同，会在相同`Deque<ProducerBatch>`的里面
3. **ProducerBatch**：表示同一个批次的消息，消息真正发送到Broker端的时候都是按照批次来发送的
4. 如果没有找到消息对应的ProducerBatch队列，则创建一个队列
5. 找到ProducerBatch队列队尾的Batch，发现Batch还可以塞下这条消息，则将消息直接塞到这个Batch中
6. 找到ProducerBatch队列队尾的Batch，发现Batch中剩余内存，不够塞下这条消息，则会创建新的Batch
7. 当消息发送成功之后，Batch会被释放掉

#### 内存分配机制

**总内存**：由`buffer.memory`控制，默认33554432（32MB）

**缓存池概念**：
- Kafka有个缓存池的概念，这个缓存池会被重复使用
- 但是只有固定(`batch.size`)的大小才能够使用缓存池
- 默认`batch.size=16384`（16KB）

##### 场景1：内存16K，缓存池中有可用内存

```
缓存池（Pool）
┌──────────────────────────────┐
│ ByteBuffer(16K) → 可用       │
│ ByteBuffer(16K) → 可用       │
│ ByteBuffer(16K) → 可用       │
└──────────────────────────────┘
         ↓ 创建Batch时获取
┌──────────────────────────────┐
│ ProducerBatch (16K)          │
└──────────────────────────────┘
         ↓ 消息发送完成
┌──────────────────────────────┐
│ ByteBuffer(16K) → 归还缓存池 │
└──────────────────────────────┘
```

**流程**：
1. 创建Batch的时候，会去缓存池中，获取队首的一块内存ByteBuffer使用
2. 消息发送完成，释放Batch，则会把这个ByteBuffer，放到缓存池的队尾中
3. 并且调用`ByteBuffer.clear`清空数据，以便下次重复使用

##### 场景2：内存16K，缓存池中无可用内存

```
缓存池（Pool）：空
非缓存池内存（nonPooledAvailableMemory）：32MB

创建Batch时：
1. 从非缓存池中分配16K内存
2. nonPooledAvailableMemory -= 16K

消息发送完成：
1. ByteBuffer归还到缓存池队尾
2. nonPooledAvailableMemory不变
```

**流程**：
1. 创建Batch的时候，去非缓存池中的内存获取一部分内存用于创建Batch
2. 注意：这里说的获取内存给Batch，其实就是让非缓存池`nonPooledAvailableMemory`减少16K的内存
3. 然后Batch正常创建就行了，**不要误以为好像真的发生了内存的转移**
4. 消息发送完成，释放Batch，则会把这个ByteBuffer，放到缓存池的队尾中
5. 并且调用`ByteBuffer.clear`清空数据，以便下次重复使用

##### 场景3：内存非16K，非缓存池中内存够用

```
需要创建48K的Batch（消息很大）

非缓存池内存（nonPooledAvailableMemory）：100MB
         ↓ 分配48K
非缓存池内存：100MB - 48K

消息发送完成：
         ↓ 释放48K
非缓存池内存：100MB（恢复）

注意：这个Batch会被GC掉，不会进入缓存池
```

**流程**：
1. 创建Batch的时候，去非缓存池(`nonPooledAvailableMemory`)内存获取一部分内存用于创建Batch
2. 注意：这里说的获取内存给Batch，其实就是让非缓存池(`nonPooledAvailableMemory`)**减少**对应的内存
3. 然后Batch正常创建就行了，**不要误以为好像真的发生了内存的转移**
4. 消息发送完成，释放Batch，纯粹的是在非缓存池(`nonPooledAvailableMemory`)中加上刚刚释放的Batch内存大小
5. 当然这个Batch会被GC掉

##### 场景4：内存非16K，非缓存池内存不够用

```
需要创建48K的Batch

缓存池：
┌──────────────────────────────┐
│ ByteBuffer(16K) → 可用       │
│ ByteBuffer(16K) → 可用       │
│ ByteBuffer(16K) → 可用       │
└──────────────────────────────┘

非缓存池内存：0KB（不够）

处理流程：
1. 释放缓存池第1个ByteBuffer(16K) → 非缓存池+16K
2. 释放缓存池第2个ByteBuffer(16K) → 非缓存池+32K
3. 释放缓存池第3个ByteBuffer(16K) → 非缓存池+48K ✓够了
4. 从非缓存池分配48K创建Batch
```

**流程**：
1. 先尝试将缓存池中的内存一个一个释放到非缓存池中，直到非缓存池中的内存够用于创建Batch了
2. 创建Batch的时候，去非缓存池(`nonPooledAvailableMemory`)内存获取一部分内存用于创建Batch
3. 注意：这里说的获取内存给Batch，其实就是让非缓存池(`nonPooledAvailableMemory`)**减少**对应的内存
4. 然后Batch正常创建就行了，**不要误以为好像真的发生了内存的转移**
5. 消息发送完成，释放Batch，纯粹的是在非缓存池(`nonPooledAvailableMemory`)中加上刚刚释放的Batch内存大小
6. 当然这个Batch会被GC掉

#### 关键配置参数

```properties
# 缓冲区总大小
buffer.memory=33554432  # 32MB

# 单个Batch大小
batch.size=16384  # 16KB

# 等待时间（凑batch）
linger.ms=0  # 默认0ms，立即发送
# 建议设置10-100ms，提高吞吐量

# 压缩类型
compression.type=none  # none/gzip/snappy/lz4/zstd
# 推荐lz4，压缩比和速度平衡
```

### 5.2 幂等性保证

#### 为什么需要幂等性？

在分布式系统中，出现网络分区是不可避免的：
- 如果Kafka broker在回复ack时，出现网络故障或者是full gc导致ack timeout
- Producer将会重发消息
- 如何保证producer重试时不造成重复or乱序？

#### 单会话幂等性

**开启方式**：
```properties
enable.idempotence=true
```

**原理**：
- Producer中每个RecordBatch都有一个单调递增的**seq（序列号）**
- Broker上每个TopicPartition也会维护**pid-seq**的映射
- 并且每Commit都会更新**lastSeq**

**去重机制**：
```
Producer发送消息：
┌─────────────────────────────┐
│ PID: 12345                  │
│ Partition: 0                │
│ Seq: 0, 1, 2, 3, 4          │
└─────────────────────────────┘
         ↓
Broker接收：
┌─────────────────────────────┐
│ 检查：baseSeq vs lastSeq   │
│ baseSeq = lastSeq + 1 ?     │
│ 是 → 保存数据               │
│ 否 → 拒绝（重复或乱序）     │
└─────────────────────────────┘
```

**工作流程**：
1. RecordBatch到来时，broker会先检查RecordBatch再保存数据
2. 如果batch中`baseSeq`（第一条消息的seq）比Broker维护的序号(`lastSeq`)大1，则保存数据
3. 否则不保存（`inSequence`方法）

**示例**：
```
假设有5个请求：batch1、batch2、batch3、batch4、batch5

正常情况：
batch1(seq:0-9) → 保存 → lastSeq=9
batch2(seq:10-19) → 保存 → lastSeq=19
batch3(seq:20-29) → 保存 → lastSeq=29

异常情况（batch2 ack失败）：
batch1(seq:0-9) → 保存 → lastSeq=9
batch2(seq:10-19) → ack失败
batch3(seq:20-29) → 拒绝（seq不连续）
batch2(seq:10-19) → 重试 → 保存 → lastSeq=19
batch3(seq:20-29) → 保存 → lastSeq=29
```

#### 对有序性的处理

**问题**：
- 假设只有batch2 ack failed，3、4、5都保存了
- 那2将会随下次batch重发而造成重复

**旧方案**：
```properties
# 设置为1，保证顺序但降低吞吐
max.in.flight.requests.per.connection=1
```

**新方案（开启幂等性后）**：
```properties
enable.idempotence=true
# 可以动态调整max.in.flight.requests.per.connection
```

**工作机制**：
1. 正常情况下`max.in.flight.requests.per.connection`大于1
2. 当重试请求到来且时，batch会根据seq重新添加到队列的合适位置
3. 并把`max.in.flight.requests.per.connection`设为1
4. 这样它前面的batch序号都比它小，只有前面的都发完了，它才能发

#### 限制

**单会话幂等性的限制**：
1. **只能保证单分区的幂等性**：不同分区的消息无法保证
2. **只能保证单会话的幂等性**：Producer重启后PID变化，无法去重
3. **无法跨会话保证**：新的Producer实例无法识别旧实例的消息

**解决方案**：使用**事务**来实现跨分区、跨会话的Exactly-Once

### 5.3 事务机制

#### 事务配置

```properties
# 开启幂等性（事务自动开启）
enable.idempotence=true

# 事务ID（必须设置）
transactional.id=my-transactional-id
```

#### 事务使用

```java
Properties props = new Properties();
props.put("bootstrap.servers", "localhost:9092");
props.put("transactional.id", "my-transactional-id");
props.put("enable.idempotence", "true");

KafkaProducer<String, String> producer = new KafkaProducer<>(props);

try {
    // 初始化事务
    producer.initTransactions();
    
    // 开启事务
    producer.beginTransaction();
    
    // 发送消息
    producer.send(new ProducerRecord<>("topic", "key", "value1"));
    producer.send(new ProducerRecord<>("topic", "key", "value2"));
    
    // 提交事务
    producer.commitTransaction();
} catch (Exception e) {
    // 回滚事务
    producer.abortTransaction();
}
```

#### 事务原理

**核心组件**：
- **Transaction Coordinator**：事务协调者，管理事务状态
- **Transaction Log**：事务日志，记录事务状态（`__transaction_state` Topic）

**工作流程**：
```
1. Producer初始化事务
   ↓
2. Producer开启事务
   ↓
3. Producer发送消息到多个分区
   ↓
4. Producer提交事务
   ↓
5. Transaction Coordinator写入事务提交标记
   ↓
6. Consumer只读取已提交的消息
```

**Consumer端配置**：
```properties
# 只读取已提交的消息
isolation.level=read_committed
```

---

## 六、消费者原理

### 6.1 Consumer Group（消费者组）

#### 核心概念

**Consumer Group**：多个消费者组成的逻辑组，是Kafka实现单播和广播两种消息模型的手段。

**特点**：
1. **同一个Topic的数据，会广播给不同的Group**
2. **同一个Group中的Worker，只有一个Worker能拿到这个数据**
3. **对于同一个Topic，每个Group都可以拿到同样的所有数据**
4. **数据进入Group后只能被其中的一个Worker消费**

#### 消费模式

```
Topic: order-topic (4个分区)

Consumer Group A (2个消费者)     Consumer Group B (4个消费者)
┌─────────────────────────┐      ┌─────────────────────────┐
│ Consumer 1 → Part 0,1   │      │ Consumer 1 → Part 0     │
│ Consumer 2 → Part 2,3   │      │ Consumer 2 → Part 1     │
│                         │      │ Consumer 3 → Part 2     │
│                         │      │ Consumer 4 → Part 3     │
└─────────────────────────┘      └─────────────────────────┘
      ↑ 单播模式                        ↑ 单播模式

Consumer Group C (1个消费者)
┌─────────────────────────┐
│ Consumer 1 → Part 0,1,2,3│
└─────────────────────────┘
      ↑ 广播模式（不同Group）
```

**重要规则**：
1. **同一分区只能被同一消费者组的一个消费者消费**
2. **不同消费者组可以消费同一分区**（广播模式）
3. **消费者数 > 分区数，部分消费者空闲**
4. **消费者数 < 分区数，一个消费者消费多个分区**

### 6.2 Rebalance机制

#### 什么是Rebalance？

**Rebalance**（再均衡）：消费者组内分区重新分配的过程。

**特点**：
- **Rebalance期间，整个消费者组停止消费**（类似JVM的STW）
- 这是Kafka最大的性能瓶颈之一

#### 触发时机

**三种情况会触发Rebalance**：

**1. 消费者数量变化**：
- Consumer Group中新增或删除某个Consumer
- 导致其所消费的分区需要分配到组内其他的Consumer上

**2. Topic发生变化**：
- Consumer订阅的Topic发生变化
- 比如订阅的Topic采用的是正则表达式的形式，如`test-*`
- 此时如果有一个新建了一个Topic `test-user`
- 那么这个Topic的所有分区也是会自动分配给当前的Consumer的

**3. 分区数量变化**：
- Consumer所订阅的Topic发生了新增分区的行为
- 那么新增的分区就会分配给当前的Consumer

#### Rebalance流程

```
1. 消费者发现Rebalance（心跳响应中包含Rebalance信号）
   ↓
2. 所有消费者停止消费，提交offset
   ↓ STW（Stop The World）
   
3. 消费者向Coordinator发送JoinGroup请求
   ↓
4. Coordinator选举一个Consumer作为Leader
   ↓
5. Leader执行分区分配策略
   （RangeAssignor/RoundRobinAssignor/StickyAssignor）
   ↓
6. Leader将分配结果发送给Coordinator
   ↓
7. Coordinator通知所有消费者最新的分区分配
   ↓
8. 消费者从新分区的offset开始消费
```

### 6.3 分区分配策略

#### 1. RangeAssignor（默认）

**原理**：按范围分配，首先会计算每个Consumer可以消费的分区个数，然后按照顺序将指定个数范围的分区分配给各个Consumer。

**示例**：
```
Topic有7个分区，3个消费者：

计算：7 / 3 = 2 余 1

分配结果：
Consumer 0: [0, 1, 2]  # 7/3=2余1，第一个消费者多分配1个
Consumer 1: [3, 4]
Consumer 2: [5, 6]
```

**缺点**：多Topic时可能不均衡
```
Topic A (7个分区) + Topic B (7个分区)，3个消费者：

Consumer 0: A[0,1,2] + B[0,1,2] = 6个分区
Consumer 1: A[3,4] + B[3,4] = 4个分区
Consumer 2: A[5,6] + B[5,6] = 4个分区

不均衡！
```

#### 2. RoundRobinAssignor（轮询）

**原理**：采用轮询的方式将当前所有的分区依次分配给所有的Consumer。

**示例**：
```
Topic有7个分区，3个消费者：

分配结果（轮询）：
Consumer 0: [0, 3, 6]
Consumer 1: [1, 4]
Consumer 2: [2, 5]

更均衡！
```

**优点**：更均衡

#### 3. StickyAssignor（粘性）⭐推荐

**原理**：实现了两个目的：
1. **将现有的分区尽可能均衡的分配给各个Consumer**
2. **如果发生Rebalance，那么重新分配之后会尽力保证当前未宕机的Consumer所消费的分区不会被分配给其他的Consumer上**

**示例**：
```
初始状态：
Consumer 0: [0, 2, 4]
Consumer 1: [1, 3]
Consumer 2: [5, 6]

Consumer 1宕机后：

RoundRobin方式：
所有分区重新分配（慢）
Consumer 0: [0, 2, 4, 6]
Consumer 2: [1, 3, 5]

Sticky方式：
只重新分配Consumer 1的分区（快）
Consumer 0: [0, 2, 4, 1]  # 保留原有+新增1
Consumer 2: [5, 6, 3]      # 保留原有+新增3
```

**优点**：
- Rebalance时尽量保持原有分配
- 减少数据迁移
- 提高Rebalance效率

### 6.4 如何避免频繁Rebalance？

**频繁Rebalance的危害**：
- 整个消费者组停止消费（STW）
- 消费延迟增加
- 吞吐量下降

**优化方案**：

#### 1. 增大心跳超时时间

```properties
# 心跳间隔（默认3秒）
heartbeat.interval.ms=3000

# 会话超时时间（默认10秒，建议30秒）
session.timeout.ms=30000
# 如果超过10s没有收到消费者的心跳，则会把消费者剔除消费组
```

#### 2. 增大处理超时时间

```properties
# 两次poll()间隔的最大时间（默认5分钟，建议10分钟）
max.poll.interval.ms=600000
# 如果两次poll的时间间隔超过这个值，集群会认为该消费者的消费能力过弱
# 将其踢出消费者组，触发rebalance机制
```

#### 3. 减少单次处理数量

```properties
# 一次poll最大拉取消息的条数（默认500，建议100-200）
max.poll.records=100
# 避免一次拉取太多消息，处理不完超时
```

#### 4. 消费者实例数 <= 分区数

```
# 多余的消费者会频繁触发Rebalance
分区数：4
消费者数：4（合理）或 2（合理）
消费者数：5（不合理，1个消费者空闲）
```

### 6.5 Offset管理

#### Offset存储位置

**Kafka 0.9之前**：存储在ZooKeeper
**Kafka 0.9之后**：存储在Kafka内部Topic（`__consumer_offsets`，50个分区）

**优点**：
1. 减轻ZooKeeper压力
2. Offset提交也是一条消息，性能更好

#### 提交方式

##### 1. 自动提交（默认）

```java
// 配置
props.put("enable.auto.commit", "true");
props.put("auto.commit.interval.ms", "5000"); // 5秒提交一次

// 代码
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
    for (ConsumerRecord<String, String> record : records) {
        System.out.printf("收到消息：partition = %d, offset = %d, value = %s%n",
            record.partition(), record.offset(), record.value());
    }
    // 自动提交，无需手动调用
}
```

**缺点**：
- 可能丢消息：消费后3秒程序崩溃，offset还没提交 → 重复消费
- 可能重复消费：提交offset后，消息还没处理完就崩溃 → 消息丢失

##### 2. 手动同步提交（推荐）

```java
// 配置
props.put("enable.auto.commit", "false");

// 代码
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
    for (ConsumerRecord<String, String> record : records) {
        System.out.printf("收到消息：partition = %d, offset = %d, value = %s%n",
            record.partition(), record.offset(), record.value());
        // 处理消息
        processRecord(record);
    }
    
    if (records.count() > 0) {
        // 手动同步提交offset，当前线程会阻塞到offset提交成功
        consumer.commitSync();
    }
}
```

**优点**：
- 精确控制offset提交时机
- 业务处理成功后再提交

**缺点**：
- 阻塞当前线程
- 降低吞吐量

##### 3. 手动异步提交

```java
// 配置
props.put("enable.auto.commit", "false");

// 代码
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
    for (ConsumerRecord<String, String> record : records) {
        processRecord(record);
    }
    
    if (records.count() > 0) {
        // 手动异步提交offset，当前线程提交offset不会阻塞
        consumer.commitAsync(new OffsetCommitCallback() {
            @Override
            public void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets,
                                   Exception exception) {
                if (exception != null) {
                    System.err.println("Commit failed for " + offsets);
                    System.err.println("Commit failed exception: " + exception.getStackTrace());
                }
            }
        });
    }
}
```

**优点**：
- 不阻塞当前线程
- 提高吞吐量

**缺点**：
- 无法保证提交成功

##### 4. 同步异步组合提交（最佳实践）

```java
props.put("enable.auto.commit", "false");

try {
    while (true) {
        ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
        for (ConsumerRecord<String, String> record : records) {
            processRecord(record);
        }
        // 异步提交（提高吞吐量）
        consumer.commitAsync();
    }
} catch (Exception exception) {
    handle(exception); // 处理异常
} finally {
    // 最后一次提交使用同步阻塞式提交（保证提交成功）
    consumer.commitSync();
    consumer.close();
}
```

#### 指定Offset消费

```java
// 1. 指定分区消费
consumer.assign(Arrays.asList(new TopicPartition(TOPIC_NAME, 0)));

// 2. 从头开始消费
consumer.seekToBeginning(Arrays.asList(new TopicPartition(TOPIC_NAME, 0)));

// 3. 从指定offset消费
consumer.seek(new TopicPartition(TOPIC_NAME, 0), 10);
```

### 6.6 长轮询机制

**默认配置**：
```java
// 一次poll最大拉取消息的条数
props.put("max.poll.records", 500);

// 代码中设置长轮询时间
ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
```

**工作机制**：
1. 如果一次poll到500条消息，就直接执行for循环进行消费
2. 如果一次没有poll到500条消息，其时间在1秒内，那么长轮询继续poll
3. 如果时间到达1s，无论是否poll到了500条消息，都直接执行for循环

**注意**：
- 如果两次poll的时间间隔超过30s（`max.poll.interval.ms`）
- 集群会认为该消费者的消费能力过弱
- 将其踢出消费者组，触发rebalance机制
- 将分区分配给其他消费者

### 6.7 消费者健康状态检查

```properties
# Consumer给broker发送心跳的间隔时间（默认1秒）
heartbeat.interval.ms=1000

# Kafka如果超过10s没有收到消费者的心跳，则会把消费者剔除消费组（默认10秒）
session.timeout.ms=10000
```

**工作机制**：
1. 消费者每隔1s向Kafka集群发送心跳
2. 集群如果发现超过10s还没有发送心跳的消费者
3. 则将其剔除消费者组，触发消费者组的rebalance机制
4. 将该分区交给消费者组中的其他消费者进行消费

---

## 七、可靠性保证

### 7.1 消息不丢失（三个维度）⭐⭐⭐⭐⭐

#### 1. Producer端不丢失

**方案1：acks=all + 幂等性 + 重试**

```properties
# 所有ISR确认
acks=all

# 开启幂等性
enable.idempotence=true

# 无限重试
retries=Integer.MAX_VALUE

# 幂等性要求≤5
max.in.flight.requests.per.connection=5
```

**方案2：同步发送 + callback检查**

```java
producer.send(record, (metadata, exception) -> {
    if (exception != null) {
        // 发送失败，记录日志或重试
        log.error("Send failed", exception);
        // 写入死信队列（DLQ）
        sendToDLQ(record);
    }
}).get(); // 同步等待结果
```

#### 2. Broker端不丢失

```properties
# 1. 副本配置
replication.factor=3                # 至少3个副本
min.insync.replicas=2               # 至少2个ISR确认
unclean.leader.election.enable=false # 禁止非ISR选举

# 2. 刷盘机制（不建议频繁刷盘，影响性能）
log.flush.interval.messages=10000   # 每1万条消息刷盘
log.flush.interval.ms=1000          # 每1秒刷盘
# 注意：Kafka依赖OS的Page Cache，不建议频繁刷盘
```

#### 3. Consumer端不丢失

**方案：手动提交 + 业务处理成功后再提交**

```java
props.put("enable.auto.commit", "false");

while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord<String, String> record : records) {
        try {
            // 先处理业务
            processRecord(record);
            // 业务成功后才提交offset
            consumer.commitSync();
        } catch (Exception e) {
            // 处理失败，不提交offset，下次重新消费
            log.error("Process failed, will retry", e);
            // 或发送到死信队列
            sendToDLQ(record);
            consumer.commitSync(); // 提交当前offset，避免重复消费
        }
    }
}
```

### 7.2 消息不重复（幂等性）⭐⭐⭐⭐⭐

#### Producer端幂等性

```properties
# 开启幂等性
enable.idempotence=true
```

**原理**：
1. Producer分配一个PID（Producer ID）
2. 每条消息分配一个Sequence Number
3. Broker根据`<PID, Partition, SeqNum>`去重

**限制**：
1. 只能保证单分区、单会话的幂等性
2. Producer重启后PID变化，无法去重

#### Consumer端幂等性（业务层）

**方案1：唯一索引（数据库层去重）**

```sql
-- 创建唯一索引
CREATE UNIQUE INDEX idx_order_id ON orders(order_id);
```

```java
@Transactional
public void processOrder(String orderId, String orderData) {
    try {
        orderRepository.save(order); // 重复插入会抛异常
    } catch (DuplicateKeyException e) {
        log.warn("Order already exists: {}", orderId);
        return; // 幂等性保证
    }
}
```

**方案2：分布式锁（Redis/ZooKeeper）**

```java
String lockKey = "order:lock:" + orderId;
if (redisTemplate.opsForValue().setIfAbsent(lockKey, "1", 10, TimeUnit.SECONDS)) {
    try {
        // 处理业务
        processOrderLogic(orderId, orderData);
    } finally {
        redisTemplate.delete(lockKey);
    }
}
```

**方案3：状态机（订单状态）**

```java
// 已支付的订单不能再次支付
Order order = orderRepository.findById(orderId);
if (order.getStatus() == OrderStatus.PAID) {
    return; // 幂等性保证
}
```

### 7.3 消息顺序性

#### 保证顺序的方案

**方案1：单分区（最简单，但并发度低）**

```properties
partitions=1
```

**方案2：Key Hash（推荐）⭐⭐⭐⭐⭐**

```java
// 相同Key发送到同一分区，一个分区只被一个消费者消费
producer.send(new ProducerRecord<>("topic", orderId, orderData));
```

**方案3：单线程消费（消费者端保证顺序）**

```java
@KafkaListener(topics = "order-topic", concurrency = "1")
public void consume(ConsumerRecord<String, String> record) {
    // 单线程处理，保证顺序
}
```

**方案4：业务层顺序保证（版本号/时间戳）**

```java
@Data
public class OrderEvent {
    private String orderId;
    private Long version;    // 版本号
    private Long timestamp;  // 时间戳
    private String data;
}

// 消费时检查版本号
if (event.getVersion() <= currentVersion) {
    return; // 旧版本数据，丢弃
}
```

## 八、50道大厂面试题精选

> **说明**：以下面试题来源于互联网公开资料、牛客网、力扣等平台的面经分享，标注的公司和职级仅供参考。

### 基础题（10题）

#### 1. Kafka的核心组件有哪些？

**来源**：字节跳动-基础轮（2024年春招）  
**难度**：⭐⭐

**答案**：
Kafka的核心组件包括：
1. **Broker**：Kafka服务器，负责存储消息
2. **Topic**：消息分类，逻辑概念
3. **Partition**：Topic的物理分区，实现并行处理
4. **Replica**：分区副本，分为Leader和Follower
5. **Producer**：消息生产者
6. **Consumer**：消息消费者，组成Consumer Group
7. **ZooKeeper/KRaft**：元数据管理和协调服务（Kafka 3.0后可使用KRaft替代ZooKeeper）

#### 2. Partition和Replica的区别？

**来源**：阿里巴巴-P6（2023年社招）  
**难度**：⭐⭐

**答案**：
- **Partition（分区）**：
  - Topic的物理分区，实现并行处理和负载均衡
  - 一个Topic可以有多个Partition
  - 不同Partition可以分布在不同的Broker上
  - 目的：提高吞吐量、实现水平扩展

- **Replica（副本）**：
  - 分区的副本，实现高可用
  - 一个Partition有多个Replica（1个Leader + N个Follower）
  - 不同Replica必须分布在不同的Broker上
  - 目的：数据冗余、故障恢复

**关系**：一个Partition有多个Replica

#### 3. 什么是ISR？

**来源**：美团-L5（2024年春招）  
**难度**：⭐⭐⭐

**答案**：
**ISR（In-Sync Replicas）**：与Leader保持同步的Follower集合。

**特点**：
1. ISR包含Leader副本自身
2. 只有ISR中的副本才有资格成为新Leader
3. ISR是动态调整的集合

**判断标准**：
- 参数：`replica.lag.time.max.ms`（默认10秒）
- Follower副本落后Leader副本的时间不超过10秒，即认为在ISR中
- 超过10秒，会被踢出ISR

**作用**：
1. 保证数据不丢失（acks=all时，等待ISR所有副本确认）
2. Leader选举（只有ISR中的副本才能成为新Leader）

#### 4. Kafka为什么快？

**来源**：腾讯-T3-1（2023年社招）  
**难度**：⭐⭐⭐⭐⭐

**答案**：
Kafka高性能的原因（6个方面）：

**1. 顺序写磁盘**：
- 比随机写内存还快（600MB/s vs 100MB/s）
- 利用磁盘的顺序读写特性

**2. Page Cache（页缓存）**：
- 利用OS的页缓存，减少磁盘IO
- 使用mmap内存映射，避免用户空间和内核空间的数据拷贝

**3. 零拷贝（Zero-Copy）**：
- 使用sendfile()系统调用
- 数据从磁盘 → 内核空间 → 网卡（2次拷贝）
- 传统方式需要4次拷贝

**4. 批量发送**：
- 减少网络往返次数
- 提高网络带宽利用率

**5. 分区并行**：
- 多个分区并行读写
- 充分利用多核CPU

**6. 压缩**：
- 减少网络传输和存储
- 支持gzip/snappy/lz4/zstd

#### 5. Kafka vs RabbitMQ如何选择？

**来源**：字节跳动-基础轮（2024年春招）  
**难度**：⭐⭐⭐

**答案**：

| 对比维度 | Kafka | RabbitMQ |
|---------|-------|----------|
| **吞吐量** | 百万级/秒 ⭐⭐⭐⭐⭐ | 万级/秒 ⭐⭐⭐ |
| **延迟** | 毫秒级（10ms） | 微秒级（更低） |
| **消息堆积** | TB级 ⭐⭐⭐⭐⭐ | GB级 ⭐⭐ |
| **顺序性** | 分区内有序 ⭐⭐⭐⭐ | 队列内有序 ⭐⭐⭐⭐ |
| **消息回溯** | 支持 ⭐⭐⭐⭐⭐ | 不支持 |
| **延迟队列** | 不支持 | 原生支持 ⭐⭐⭐⭐⭐ |
| **死信队列** | 需要自己实现 | 原生支持 ⭐⭐⭐⭐⭐ |

**选择建议**：
- **选Kafka**：高吞吐量、消息堆积、日志收集、大数据、流式处理
- **选RabbitMQ**：业务解耦、延迟队列、死信队列、低延迟、复杂路由

**结合项目经验（DFN项目）**：
- 场景：1000+门店，高并发认证请求，需要高吞吐量
- 选择：Kafka（异步解耦下游系统，吞吐量提升3倍）

#### 6. 什么是Consumer Group？

**来源**：阿里巴巴-P6（2023年社招）  
**难度**：⭐⭐

**答案**：
**Consumer Group（消费者组）**：多个消费者组成的逻辑组，是Kafka实现单播和广播两种消息模型的手段。

**特点**：
1. **同一个Topic的数据，会广播给不同的Group**
2. **同一个Group中的Worker，只有一个Worker能拿到这个数据**
3. **对于同一个Topic，每个Group都可以拿到同样的所有数据**
4. **数据进入Group后只能被其中的一个Worker消费**

**重要规则**：
1. 同一分区只能被同一消费者组的一个消费者消费
2. 不同消费者组可以消费同一分区（广播模式）
3. 消费者数 > 分区数，部分消费者空闲
4. 消费者数 < 分区数，一个消费者消费多个分区

#### 7. Offset存储在哪里？

**来源**：美团-L5（2024年春招）  
**难度**：⭐⭐

**答案**：
**Kafka 0.9之前**：存储在ZooKeeper

**Kafka 0.9之后**：存储在Kafka内部Topic（`__consumer_offsets`，50个分区）

**优点**：
1. 减轻ZooKeeper压力
2. Offset提交也是一条消息，性能更好
3. Kafka自己管理offset，更加可控

**查看offset**：
```bash
kafka-consumer-groups.sh --describe \
  --bootstrap-server localhost:9092 \
  --group order-consumer-group
```

#### 8. Kafka的消息格式？

**来源**：腾讯-T3-1（2023年社招）  
**难度**：⭐⭐

**答案**：
```
Message = {
    Offset: 分区内唯一标识（8字节）
    Timestamp: 消息时间戳
    Key: 消息键（可选，用于分区路由）
    Value: 消息体（实际数据）
    Headers: 消息头（可选，类似HTTP Header，K-V结构）
}
```

**关键字段说明**：
- **Offset**：消息在分区中的位置，唯一标识一条消息
- **Key**：用于分区路由，相同Key的消息发送到同一分区
- **Value**：消息的实际内容
- **Headers**：可以存储元数据，如消息来源、重试次数等

#### 9. Topic、Partition、Segment的关系？

**来源**：字节跳动-基础轮（2024年春招）  
**难度**：⭐⭐⭐

**答案**：
```
Topic（逻辑概念）
│
├─ Partition 0（物理分区）
│  ├─ Segment 0（LogSegment）
│  │  ├─ 00000000000000000000.log
│  │  ├─ 00000000000000000000.index
│  │  └─ 00000000000000000000.timeindex
│  ├─ Segment 1
│  │  ├─ 00000000000000001000.log
│  │  ├─ 00000000000000001000.index
│  │  └─ 00000000000000001000.timeindex
│  └─ ...
│
├─ Partition 1
│  └─ ...
│
└─ Partition 2
   └─ ...
```

**关系说明**：
1. **Topic**：消息的逻辑分类
2. **Partition**：Topic的物理分区，实现并行处理
3. **Segment**：Partition的分段存储，每个Segment包含.log、.index、.timeindex文件

**Segment切分条件**（4个）：
1. 日志文件大小超过`log.segment.bytes`（默认1GB）
2. 时间差超过`log.roll.ms`或`log.roll.hours`（默认7天）
3. 索引文件大小超过`log.index.size.max.bytes`（默认10MB）
4. 相对位移量超过Integer.MAX_VALUE

#### 10. Kafka如何保证高可用？

**来源**：阿里巴巴-P6（2023年社招）  
**难度**：⭐⭐⭐⭐

**答案**：
Kafka通过以下机制保证高可用：

**1. 副本机制（Replication）**：
```properties
# 设置副本数（至少3个）
replication.factor=3
```
- 一个Partition有多个Replica（1个Leader + N个Follower）
- Leader负责读写，Follower负责同步
- Leader宕机后，从ISR中选举新Leader

**2. ISR机制**：
```properties
# 至少2个ISR副本确认
min.insync.replicas=2
```
- 只有ISR中的副本才有资格成为新Leader
- 保证数据不丢失

**3. Leader选举**：
```properties
# 禁止非ISR选举（防止数据丢失）
unclean.leader.election.enable=false
```
- Leader宕机后，从ISR中选举新Leader
- 禁止非ISR副本成为Leader，避免数据丢失

**4. Controller机制**：
- Kafka集群的"大脑"，负责管理整个集群
- 从所有Broker中选举一个作为Controller
- 负责分区Leader选举、分区副本分配、监听Broker上下线

**5. ZooKeeper/KRaft**：
- ZooKeeper：元数据管理和协调服务
- KRaft（Kafka 3.0+）：移除ZooKeeper依赖，使用Raft协议

---

### 原理题（15题）

#### 11. 详细说明ISR机制？⭐⭐⭐⭐⭐

**来源**：字节跳动-进阶轮（2024年春招）  
**难度**：⭐⭐⭐⭐⭐

**答案**：

**ISR（In-Sync Replicas）定义**：
- 与Leader保持同步的Follower集合
- ISR包含Leader副本自身
- ISR是动态调整的集合

**判断标准（0.9.0.0版本之后）**：
```properties
# Follower副本落后Leader副本的最大时间
replica.lag.time.max.ms=10000  # 默认10秒
```
- 只要在10秒内Follower有同步消息，即认为在ISR中
- 超过10秒，会被踢出ISR

**ISR动态扩缩容**：
- Kafka启动时会开启两个任务
- 任务1：定期检查是否需要缩减或扩大ISR集合（周期5秒）
  - 检测到ISR集合中有失效副本时，收缩ISR集合
  - 检查到Follower的HW追赶上Leader时，扩充ISR
- 任务2：周期性检查ISR变更记录
  - 如果发现ISR集合的变更记录，在ZK中持久化节点
  - Controller监听到变化，向Broker发送更新元数据的请求

**ISR的作用**：
1. **Leader选举范围**：
   - 只有ISR中的副本才有资格成为新Leader
   - 参数`unclean.leader.election.enable`控制是否允许非ISR选举
   
2. **生产ack=-1的保证**：
   - 生产者发送消息后，消息需要写入ISR集合中全部副本，才算提交成功
   - 如果ISR集合中只有Leader一个节点，那么-1就退化为了1

**0.9.0.0版本之前的设计**：
```properties
# 允许Follower落后Leader的消息数量
replica.lag.max.messages=4000
```
**问题**：
- 很难设置合理值
- 生产者批量发送大量消息时，会导致Follower频繁被踢出ISR
- 设置太大，broker宕机时可能导致消息严重丢失

#### 12. Leader选举机制？⭐⭐⭐⭐⭐

**来源**：阿里巴巴-P7（2023年社招）  
**难度**：⭐⭐⭐⭐⭐

**答案**：

**选举时机**：
1. Broker宕机（Controller检测到）
2. 分区Leader宕机
3. 手动触发（运维操作）

**选举策略**：
```
ISR = [Replica 0, Replica 1, Replica 2]

Leader宕机 → 从ISR中选举新Leader

选举规则：
1. 优先从ISR中选择第一个存活的Replica（AR列表顺序）
2. 如果ISR为空：
   - unclean.leader.election.enable=true → 允许非ISR选举（可能丢数据）
   - unclean.leader.election.enable=false → 等待ISR恢复（牺牲可用性）
```

**不完全的首领选举（Unclean Leader Election）**：
```properties
# 是否允许非ISR副本成为Leader
unclean.leader.election.enable=false  # 默认false
```

**true（允许非ISR选举）**：
- **优点**：提高可用性，Leader一直存在
- **缺点**：可能造成数据丢失（非ISR副本数据不完整）

**false（禁止非ISR选举）**：
- **优点**：维护数据一致性，避免消息丢失
- **缺点**：牺牲可用性，等待ISR恢复

**CAP权衡**：
- true：选择A（可用性），牺牲C（一致性）
- false：选择C（一致性），牺牲A（可用性）

**Controller的作用**：
- Kafka集群的"大脑"，负责Leader选举
- 从所有Broker中选举一个作为Controller
- 监听Broker上下线，触发Leader选举
- 通知其他Broker更新元数据

#### 13. HW和LEO的区别和作用？⭐⭐⭐⭐⭐

**来源**：美团-L6（2024年春招）  
**难度**：⭐⭐⭐⭐⭐

**答案**：

**LEO（Log End Offset）**：
- **定义**：日志末端位移，记录了该副本对象底层日志文件中下一条消息的位移值
- **更新时机**：副本写入消息的时候，会自动更新LEO值
- **作用**：标识副本的最新消息位置

**HW（High Watermark）**：
- **定义**：高水印值，HW一定不会大于LEO值
- **作用**：
  1. **定义消息可见性**：小于HW值的消息被认为是"已提交"或"已备份"的消息，并对消费者可见
  2. **保证数据一致性**：Consumer只能消费到HW之前的消息

**Remote LEO**：
- **定义**：Leader中保存的Follower副本的LEO值
- **作用**：决定HW值大小的关键，HW = min(Leader LEO, Remote LEO1, Remote LEO2, ...)

**图解**：
```
Partition Replica (Leader)
┌─────────────────────────────────┐
│  offset: 0   1   2   3   4   5  │
│  msg:   [A] [B] [C] [D] [E] [F] │
│                      ↑       ↑   │
│                     HW      LEO  │
└─────────────────────────────────┘

LEO = 6（下一条消息的offset）
HW = 3（所有ISR副本都已同步的offset）

Consumer只能消费到HW之前的消息（0-2）
HW之后的消息（3-5）还未完全同步，不可见
```

**HW更新机制**：

**Leader HW更新时机**：
1. **故障时更新**：
   - 副本被选为Leader时
   - 副本被踢出ISR时
   
2. **正常时更新**：
   - Producer向Leader写入消息时
   - Leader处理Follower FETCH请求时

**Follower HW更新时机**：
- Follower更新HW发生在其更新LEO之后
- 每次Follower Fetch响应体都会包含Leader的HW值
- 然后比较当前LEO值，取最小的作为新的HW值

**关键点**：
- Leader的Remote LEO值的更新总是需要额外一轮fetch RPC请求才能完成
- 这意味着在Leader切换过程中，会存在数据丢失以及数据不一致的问题
- 因此引入了Leader Epoch机制来解决

#### 14. Leader Epoch如何解决数据丢失？⭐⭐⭐⭐⭐

**来源**：腾讯-T3-2（2023年社招）  
**难度**：⭐⭐⭐⭐⭐

**答案**：

**问题背景**：
- HW更新时机是异步延迟的
- HW又是决定日志是否备份成功的标志
- 导致数据丢失和数据不一致的问题

**Leader Epoch是什么**：
- **格式**：`(epoch, offset)`
- **epoch**：Leader版本，单调递增的正整数，每次Leader变更，epoch+1
- **offset**：每一代Leader写入的第一条消息的位移值

**示例**：
```
(0, 0)    # 第0代Leader从位移0开始写入
(1, 300)  # 第1代Leader从位移300开始写入（第0代写入了0-299）
(2, 650)  # 第2代Leader从位移650开始写入（第1代写入了300-649）
```

**工作机制**：

**当副本成为Leader时**：
- 如果生产者有新消息发送过来
- 会首先将新的Leader epoch以及LEO添加到`leader-epoch-checkpoint`文件中

**当副本变成Follower时**：
1. 发送LeaderEpochRequest请求给Leader副本（包含Follower的最新epoch）
2. Leader返回LastOffset：
   - 如果`follower last epoch = leader last epoch`，则`LastOffset = leader LEO`
   - 否则取大于follower last epoch中最小的leader epoch的start offset值
3. Follower判断是否需要日志截断：
   - 如果当前LEO > LastOffset，则从LastOffset截断日志
4. Follower开始发送fetch请求给Leader保持消息同步

**解决数据丢失场景**：
```
场景：副本A(Follower)、副本B(Leader)

传统HW机制：
1. A重启后，根据HW进行日志截断
2. B宕机，A成为Leader
3. B重启后，根据A的HW进行日志截断
4. 结果：数据丢失

Leader Epoch机制：
1. A重启后，发送LeaderEpochRequest给B
2. B返回LastOffset=2（epoch=0相同）
3. A发现LastOffset=2等于当前LEO，不需要截断
4. B宕机，A成为Leader
5. B重启后，同样不需要截断
6. 结果：数据没有丢失！
```

**解决数据不一致场景**：
```
场景：副本A、副本B同时宕机，B先重启成为Leader

传统HW机制：
1. B成为Leader，接收新消息m2
2. A重启，发现HW相同，不截断
3. 结果：A的offset=1是m1，B的offset=1是m2，数据不一致

Leader Epoch机制：
1. B成为Leader，接收新消息m2，epoch更新为1
2. A重启，发送LeaderEpochRequest（epoch=0）给B
3. B返回LastOffset=1（找到epoch=1的start offset）
4. A发现LastOffset=1 < 当前LEO=2，从LastOffset截断
5. A开始同步B的消息
6. 结果：数据一致！
```

#### 15. 零拷贝技术原理？⭐⭐⭐⭐⭐

**来源**：字节跳动-进阶轮（2024年春招）  
**难度**：⭐⭐⭐⭐⭐

**答案**：

**传统方式（4次拷贝）**：
```
应用程序读取文件并通过网络发送：

1. 磁盘 → 内核缓冲区（DMA拷贝）
2. 内核缓冲区 → 用户空间缓冲区（CPU拷贝）
3. 用户空间缓冲区 → Socket缓冲区（CPU拷贝）
4. Socket缓冲区 → 网卡（DMA拷贝）

共4次拷贝，2次DMA拷贝 + 2次CPU拷贝
4次上下文切换（用户态 ↔ 内核态）
```

**零拷贝方式（2次拷贝）**：
```
使用sendfile()系统调用：

1. 磁盘 → 内核缓冲区（DMA拷贝）
2. 内核缓冲区 → Socket缓冲区（CPU拷贝）
3. Socket缓冲区 → 网卡（DMA拷贝）

共3次拷贝，2次DMA拷贝 + 1次CPU拷贝
2次上下文切换
```

**真正的零拷贝（Gather操作）**：
```
Linux 2.4+，支持gather操作的网卡：

1. 磁盘 → 内核缓冲区（DMA拷贝）
2. 内核缓冲区 → 网卡（DMA拷贝，直接传递指针）

共2次拷贝，全部是DMA拷贝，0次CPU拷贝
2次上下文切换
```

**Kafka中的应用**：
- Kafka使用Java NIO的`FileChannel.transferTo()`方法
- 底层调用Linux的`sendfile()`系统调用
- 实现零拷贝，避免数据在用户空间和内核空间的拷贝

**代码示例**：
```java
// Kafka中的零拷贝实现
FileChannel fileChannel = new RandomAccessFile(file, "r").getChannel();
SocketChannel socketChannel = SocketChannel.open();

// transferTo方法底层调用sendfile()
fileChannel.transferTo(position, count, socketChannel);
```

**性能提升**：
- 传统方式 vs 零拷贝方式：性能提升2-3倍
- MB到GB级别的文件传输，零拷贝优势明显

---

*（文档继续，由于篇幅限制，剩余35道面试题将在下一部分添加）*

